[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Overview",
    "section": "",
    "text": "In ESS 330 I learned R while practicing data manipulation, analysis, visualization and modeling. I also built websites using Quarto, R and GitHub."
  },
  {
    "objectID": "lab_overview.html",
    "href": "lab_overview.html",
    "title": "ESS 330 Lab Overview",
    "section": "",
    "text": "Overview of Labs\nThis document provides a summary for each lab file.\n\n\nLabs\n\nLab 01\n\nIn this lab we built personal websites using github pages. The linked file connects to the github repo for the personal site.\n\nLab 02\n\nIn this lab we began to explore data maniplation in R.\n\nLab 03\n\nIn this lab we used the NY-Times COVID-19 dataset to practice data manipulation, vizualization and EDA.\n\nLab 04\n\nIn this lab we used the LTER dataset to practice statistical analysis using R.\n\nLab 05\n\nIn this lab my group explored some potential project topics for the course project.\n\nLab 06\n\nIn this lab we practiced building machine learning models using the CAMELS hyrological dataset.\n\nLab 07\n\nIn this lab my group outlined our course project with an abstract and exploratory data analysis (EDA).\n\nLab 08\n\nIn this lab we practiced tuning the best machine learning model from Lab 06 using the CAMELS hyrological dataset.\n\nLab 09\n\nIn this lab my group completed the modeling for our project and vizualized the results.\n\nLab 10\n\nIn this lab we work with spatial data, exploring the distance form borders in colorado and nationally."
  },
  {
    "objectID": "labs/lab_08.html",
    "href": "labs/lab_08.html",
    "title": "Lab 8: Tuning ML Models of Hydrological Data",
    "section": "",
    "text": "library(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.7     ✔ recipes      1.1.1\n✔ dials        1.3.0     ✔ rsample      1.3.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.1     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.2\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.4.3\n\nlibrary(ggplot2)\nlibrary(workflowsets)\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\nlibrary(ggfortify)\n\nWarning: package 'ggfortify' was built under R version 4.4.3\n\n\nRegistered S3 method overwritten by 'ggfortify':\n  method          from   \n  autoplot.glmnet parsnip\n\nlibrary(parsnip)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.4     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(visdat)\n\nWarning: package 'visdat' was built under R version 4.4.3\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.4.3\n\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(patchwork)\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\n# Data Import/Tidy/Transform    \nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\nWarning in\ndownload.file(\"https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf\",\n: URL https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf:\ncannot open destfile 'data/camels_attributes_v2.0.pdf', reason 'No such file or\ndirectory'\n\n\nWarning in\ndownload.file(\"https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf\",\n: download had nonzero exit status\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\n\nlocal_files   &lt;- glue('../data/lab_data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \ncamels &lt;- power_full_join(camels ,by = 'gauge_id') \n\n# Add log(q_mean) to df\ncamels &lt;- camels %&gt;% \n  mutate(logQmean = log(q_mean)) %&gt;% \n  mutate(across(everything(), as.double))\n\nWarning: There were 5 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(everything(), as.double)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 4 remaining warnings.\n\nskim(camels)\n\n\nData summary\n\n\nName\ncamels\n\n\nNumber of rows\n671\n\n\nNumber of columns\n59\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n59\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ngauge_id\n0\n1.00\n6265830.84\n3976867.52\n1013500.00\n2370650.00\n6278300.00\n9382765.00\n14400000.00\n▇▃▅▃▃\n\n\np_mean\n0\n1.00\n3.26\n1.41\n0.64\n2.37\n3.23\n3.78\n8.94\n▃▇▂▁▁\n\n\npet_mean\n0\n1.00\n2.79\n0.55\n1.90\n2.34\n2.69\n3.15\n4.74\n▇▇▅▂▁\n\n\np_seasonality\n0\n1.00\n-0.04\n0.53\n-1.44\n-0.26\n0.08\n0.22\n0.92\n▁▂▃▇▂\n\n\nfrac_snow\n0\n1.00\n0.18\n0.20\n0.00\n0.04\n0.10\n0.22\n0.91\n▇▂▁▁▁\n\n\naridity\n0\n1.00\n1.06\n0.62\n0.22\n0.70\n0.86\n1.27\n5.21\n▇▂▁▁▁\n\n\nhigh_prec_freq\n0\n1.00\n20.93\n4.55\n7.90\n18.50\n22.00\n24.23\n32.70\n▂▃▇▇▁\n\n\nhigh_prec_dur\n0\n1.00\n1.35\n0.19\n1.08\n1.21\n1.28\n1.44\n2.09\n▇▅▂▁▁\n\n\nhigh_prec_timing\n671\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nlow_prec_freq\n0\n1.00\n254.65\n35.12\n169.90\n232.70\n255.85\n278.92\n348.70\n▂▅▇▅▁\n\n\nlow_prec_dur\n0\n1.00\n5.95\n3.20\n2.79\n4.24\n4.95\n6.70\n36.51\n▇▁▁▁▁\n\n\nlow_prec_timing\n671\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\ngeol_1st_class\n671\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nglim_1st_class_frac\n0\n1.00\n0.79\n0.20\n0.30\n0.61\n0.83\n1.00\n1.00\n▁▃▃▃▇\n\n\ngeol_2nd_class\n671\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nglim_2nd_class_frac\n0\n1.00\n0.16\n0.14\n0.00\n0.00\n0.14\n0.27\n0.49\n▇▃▃▂▁\n\n\ncarbonate_rocks_frac\n0\n1.00\n0.12\n0.26\n0.00\n0.00\n0.00\n0.04\n1.00\n▇▁▁▁▁\n\n\ngeol_porostiy\n3\n1.00\n0.13\n0.07\n0.01\n0.07\n0.13\n0.19\n0.28\n▇▆▇▇▂\n\n\ngeol_permeability\n0\n1.00\n-13.89\n1.18\n-16.50\n-14.77\n-13.96\n-13.00\n-10.90\n▂▅▇▅▂\n\n\nsoil_depth_pelletier\n0\n1.00\n10.87\n16.24\n0.27\n1.00\n1.23\n12.89\n50.00\n▇▁▁▁▁\n\n\nsoil_depth_statsgo\n0\n1.00\n1.29\n0.27\n0.40\n1.11\n1.46\n1.50\n1.50\n▁▁▂▂▇\n\n\nsoil_porosity\n0\n1.00\n0.44\n0.02\n0.37\n0.43\n0.44\n0.46\n0.68\n▃▇▁▁▁\n\n\nsoil_conductivity\n0\n1.00\n1.74\n1.52\n0.45\n0.93\n1.35\n1.93\n13.96\n▇▁▁▁▁\n\n\nmax_water_content\n0\n1.00\n0.53\n0.15\n0.09\n0.43\n0.56\n0.64\n1.05\n▁▅▇▃▁\n\n\nsand_frac\n0\n1.00\n36.47\n15.63\n8.18\n25.44\n35.27\n44.46\n91.98\n▅▇▅▁▁\n\n\nsilt_frac\n0\n1.00\n33.86\n13.25\n2.99\n23.95\n34.06\n43.64\n67.77\n▂▆▇▆▁\n\n\nclay_frac\n0\n1.00\n19.89\n9.32\n1.85\n14.00\n18.66\n25.42\n50.35\n▃▇▅▂▁\n\n\nwater_frac\n0\n1.00\n0.10\n0.94\n0.00\n0.00\n0.00\n0.00\n19.35\n▇▁▁▁▁\n\n\norganic_frac\n0\n1.00\n0.59\n3.84\n0.00\n0.00\n0.00\n0.00\n57.86\n▇▁▁▁▁\n\n\nother_frac\n0\n1.00\n9.82\n16.83\n0.00\n0.00\n1.31\n11.74\n99.38\n▇▁▁▁▁\n\n\ngauge_lat\n0\n1.00\n39.24\n5.21\n27.05\n35.70\n39.25\n43.21\n48.82\n▂▃▇▆▅\n\n\ngauge_lon\n0\n1.00\n-95.79\n16.21\n-124.39\n-110.41\n-92.78\n-81.77\n-67.94\n▆▃▇▇▅\n\n\nelev_mean\n0\n1.00\n759.42\n786.00\n10.21\n249.67\n462.72\n928.88\n3571.18\n▇▂▁▁▁\n\n\nslope_mean\n0\n1.00\n46.20\n47.12\n0.82\n7.43\n28.80\n73.17\n255.69\n▇▂▂▁▁\n\n\narea_gages2\n0\n1.00\n792.62\n1701.95\n4.03\n122.28\n329.68\n794.30\n25791.04\n▇▁▁▁▁\n\n\narea_geospa_fabric\n0\n1.00\n808.08\n1709.85\n4.10\n127.98\n340.70\n804.50\n25817.78\n▇▁▁▁▁\n\n\nfrac_forest\n0\n1.00\n0.64\n0.37\n0.00\n0.28\n0.81\n0.97\n1.00\n▃▁▁▂▇\n\n\nlai_max\n0\n1.00\n3.22\n1.52\n0.37\n1.81\n3.37\n4.70\n5.58\n▅▆▃▅▇\n\n\nlai_diff\n0\n1.00\n2.45\n1.33\n0.15\n1.20\n2.34\n3.76\n4.83\n▇▇▇▆▇\n\n\ngvf_max\n0\n1.00\n0.72\n0.17\n0.18\n0.61\n0.78\n0.86\n0.92\n▁▁▂▃▇\n\n\ngvf_diff\n0\n1.00\n0.32\n0.15\n0.03\n0.19\n0.32\n0.46\n0.65\n▃▇▅▇▁\n\n\ndom_land_cover_frac\n0\n1.00\n0.81\n0.18\n0.31\n0.65\n0.86\n1.00\n1.00\n▁▂▃▃▇\n\n\ndom_land_cover\n671\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nroot_depth_50\n24\n0.96\n0.18\n0.03\n0.12\n0.17\n0.18\n0.19\n0.25\n▃▃▇▂▂\n\n\nroot_depth_99\n24\n0.96\n1.83\n0.30\n1.50\n1.52\n1.80\n2.00\n3.10\n▇▃▂▁▁\n\n\nq_mean\n1\n1.00\n1.49\n1.54\n0.00\n0.63\n1.13\n1.75\n9.69\n▇▁▁▁▁\n\n\nrunoff_ratio\n1\n1.00\n0.39\n0.23\n0.00\n0.24\n0.35\n0.51\n1.36\n▆▇▂▁▁\n\n\nslope_fdc\n1\n1.00\n1.24\n0.51\n0.00\n0.90\n1.28\n1.63\n2.50\n▂▅▇▇▁\n\n\nbaseflow_index\n0\n1.00\n0.49\n0.16\n0.01\n0.40\n0.50\n0.60\n0.98\n▁▃▇▅▁\n\n\nstream_elas\n1\n1.00\n1.83\n0.78\n-0.64\n1.32\n1.70\n2.23\n6.24\n▁▇▃▁▁\n\n\nq5\n1\n1.00\n0.17\n0.27\n0.00\n0.01\n0.08\n0.22\n2.42\n▇▁▁▁▁\n\n\nq95\n1\n1.00\n5.06\n4.94\n0.00\n2.07\n3.77\n6.29\n31.82\n▇▂▁▁▁\n\n\nhigh_q_freq\n1\n1.00\n25.74\n29.07\n0.00\n6.41\n15.10\n35.79\n172.80\n▇▂▁▁▁\n\n\nhigh_q_dur\n1\n1.00\n6.91\n10.07\n0.00\n1.82\n2.85\n7.55\n92.56\n▇▁▁▁▁\n\n\nlow_q_freq\n1\n1.00\n107.62\n82.24\n0.00\n37.44\n96.00\n162.14\n356.80\n▇▆▅▂▁\n\n\nlow_q_dur\n1\n1.00\n22.28\n21.66\n0.00\n10.00\n15.52\n26.91\n209.88\n▇▁▁▁▁\n\n\nzero_q_freq\n1\n1.00\n0.03\n0.11\n0.00\n0.00\n0.00\n0.00\n0.97\n▇▁▁▁▁\n\n\nhfd_mean\n1\n1.00\n182.52\n33.53\n112.25\n160.16\n173.77\n204.05\n287.75\n▂▇▃▂▁\n\n\nlogQmean\n1\n1.00\n-0.11\n1.17\n-5.39\n-0.46\n0.12\n0.56\n2.27\n▁▁▂▇▂\n\n\n\n\nvis_dat(camels)\n\n\n\n\n\n\n\n\n\n# Set seed\nset.seed(567)\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_tr &lt;- training(camels_split)\ncamels_te  &lt;- testing(camels_split)\n\n# Cross-validation folds\ncamels_10cv &lt;- vfold_cv(camels_tr, v = 10)\n\n# Recipe\nrec &lt;- recipe(logQmean ~ pet_mean + p_mean + aridity + runoff_ratio + baseflow_index + slope_mean + area_geospa_fabric, data = camels_tr) %&gt;% \n  step_YeoJohnson(all_predictors()) %&gt;% \n  step_interact(terms = ~ pet_mean:p_mean + aridity:runoff_ratio + area_geospa_fabric:slope_mean) %&gt;% \n  step_corr(all_predictors(), threshold = 0.9) %&gt;%   # Remove highly correlated predictors to avoid multicollinearity.\n  step_normalize(all_predictors()) %&gt;% \n  step_naomit(all_predictors(), all_outcomes())\n\n# Define and Train Models\n  ## Define rf model\n  rf_model &lt;- rand_forest() %&gt;% \n    set_engine(\"ranger\") %&gt;% \n    set_mode(\"regression\")\n  \n  rf_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(rec) %&gt;%\n    # Add the model\n    add_model(rf_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n   \n  rf_predictions &lt;- augment(rf_wf, new_data = camels_te) \n\n  ## Define xg model\n  xg_model &lt;- boost_tree() %&gt;% \n    set_engine(\"xgboost\") %&gt;% \n    set_mode(\"regression\")\n  \n  xg_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(rec) %&gt;%\n    # Add the model\n    add_model(xg_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n  \n  xg_predictions &lt;- augment(xg_wf, new_data = camels_te)\n  \n  ## Define nueral net model\n  nn_model &lt;- bag_mlp() %&gt;% \n    set_engine(\"nnet\") %&gt;% \n    set_mode(\"regression\")\n  \n  nn_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(rec) %&gt;%\n    # Add the model\n    add_model(nn_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n  \n  nn_predictions &lt;- augment(nn_wf, new_data = camels_te)\n  \n  ## Define linear reg model\n  lm_model &lt;- linear_reg() %&gt;% \n    set_engine(\"lm\") %&gt;% \n    set_mode(\"regression\")\n  \n  lm_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(rec) %&gt;%\n    # Add the model\n    add_model(lm_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n  \n  lm_predictions &lt;- augment(lm_wf, new_data = camels_te) \n  \n  # Implement workflowset analysis\n  \n  ml_wf_set &lt;- workflow_set(preproc = list(rec),\n                          models = list(rf = rf_model, \n                                        xg = xg_model, \n                                        nn = nn_model, \n                                        lm = lm_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_10cv) \n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(ml_wf_set)\n\n\n\n\n\n\n\nrank_results(ml_wf_set, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id  .config        .metric   mean std_err     n preprocessor model  rank\n  &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_nn Preprocessor1… rmse    0.0304 1.04e-2    10 recipe       bag_…     1\n2 recipe_nn Preprocessor1… rsq     0.999  4.79e-4    10 recipe       bag_…     1\n3 recipe_xg Preprocessor1… rmse    0.122  1.61e-2    10 recipe       boos…     2\n4 recipe_xg Preprocessor1… rsq     0.989  2.04e-3    10 recipe       boos…     2\n5 recipe_rf Preprocessor1… rmse    0.175  2.43e-2    10 recipe       rand…     3\n6 recipe_rf Preprocessor1… rsq     0.981  3.14e-3    10 recipe       rand…     3\n7 recipe_lm Preprocessor1… rmse    0.217  2.40e-2    10 recipe       line…     4\n8 recipe_lm Preprocessor1… rsq     0.967  4.71e-3    10 recipe       line…     4\n\n\n\n# model tuning\ntuned_nn_model &lt;- bag_mlp(\n  hidden_units = tune(), \n  penalty = tune()\n) %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nwf_tune &lt;- workflow() %&gt;% \n  add_recipe(rec) %&gt;% \n  add_model(tuned_nn_model)\n\ndials &lt;- extract_parameter_set_dials(wf_tune)\n\n# define search space\nmy.grid &lt;- grid_space_filling(dials, size = 20)\n\nmodel_params &lt;-  tune_grid(\n    wf_tune,\n    resamples = camels_10cv,\n    grid = my.grid,\n    metrics = metric_set(rmse, rsq, mae),\n    control = control_grid(save_pred = TRUE)\n  )\nautoplot(model_params)\n\n\n\n\n\n\n\ncollect_metrics(model_params)\n\n# A tibble: 60 × 8\n   hidden_units       penalty .metric .estimator   mean     n std_err .config   \n          &lt;int&gt;         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;     \n 1            1 0.000000144   mae     standard   0.104     10 0.00738 Preproces…\n 2            1 0.000000144   rmse    standard   0.173     10 0.0246  Preproces…\n 3            1 0.000000144   rsq     standard   0.980     10 0.00397 Preproces…\n 4            1 0.0000616     mae     standard   0.109     10 0.00715 Preproces…\n 5            1 0.0000616     rmse    standard   0.174     10 0.0242  Preproces…\n 6            1 0.0000616     rsq     standard   0.979     10 0.00393 Preproces…\n 7            2 0.0264        mae     standard   0.0399    10 0.00453 Preproces…\n 8            2 0.0264        rmse    standard   0.0824    10 0.0205  Preproces…\n 9            2 0.0264        rsq     standard   0.995     10 0.00191 Preproces…\n10            2 0.00000000113 mae     standard   0.0339    10 0.00442 Preproces…\n# ℹ 50 more rows\n\nbest_mae &lt;- show_best(model_params, metric = \"mae\", n = 1)\nhp_best &lt;- select_best(model_params, metric = \"mae\")\n#&gt; The first row shows the mean MAE across resamples, SE of the MAE estimate, # of resamples, and mean SE. Penalty is the best hyperparameter set for this model. \n\nfinal_wf &lt;- finalize_workflow(wf_tune, hp_best)\nfinal_fit &lt;- last_fit(final_wf, split = camels_split)\nfinal_metrics &lt;- collect_metrics(final_fit)\n\n# The final model's rmse 0.010 and the rsq is 0.999. This means that 99.9% of the variance is explained by the model. This is an excellent number and result. The rmse is the average prediction error, and this percentage is ~1% which is quite good. This model is very good, but may be less efficient than more simple models which are less demanding computationally with worse but still acceptable values for rsq and rmse.\n\npredictions &lt;- collect_predictions(model_params)\n\nggplot(predictions, aes(x = .pred, y = logQmean)) +\n  geom_smooth(method = lm, color = \"blue\") +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  scale_color_gradient() +\n  labs(\n    title = \"Actual vs. Predicted Values\", \n    x = \"Predicted\", \n    y = \"Actual\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfinal_fit_full &lt;- fit(final_wf, data = camels)\naugmented_preds &lt;- augment(final_fit_full, new_data = camels)\n\naugmented_preds &lt;- augmented_preds %&gt;% \n  mutate(residual_sq = (logQmean - .pred)^2)\n\nmap_preds &lt;- ggplot(augmented_preds, aes(x = .pred, y = logQmean)) +\n  geom_point(aes(color = .pred), size = 3, alpha = 0.8) +\n  scale_color_viridis_c(name = \"Predicted\") +\n  coord_fixed() +\n  labs(title = \"Map of Predicted logQmean\") +\n  theme_minimal()\n\nmap_resid &lt;- ggplot(augmented_preds, aes(x = .pred, y = residual_sq)) +\n  geom_point() +\n  scale_color_viridis_c(name = \"Residual²\") +\n  labs(title = \"Map of Squared Residuals\") +\n  theme_minimal()\n\nmaps_combined &lt;- map_preds | map_resid\n\nprint(maps_combined)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "labs/lab_04.html",
    "href": "labs/lab_04.html",
    "title": "Lab 4: LTER Network Data",
    "section": "",
    "text": "# Install LTER Data Sampler\n#remotes::install_github(\"lter/lterdatasampler\")\n\n# Install necessary libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3\n\nlibrary(lterdatasampler)\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.4.3\n\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.4.3\n\n\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nlibrary(visdat)\n\nWarning: package 'visdat' was built under R version 4.4.3\n\nlibrary(broom)\nlibrary(flextable)\n\n\nAttaching package: 'flextable'\n\nThe following objects are masked from 'package:ggpubr':\n\n    border, font, rotate\n\nThe following object is masked from 'package:purrr':\n\n    compose\n\n# Retrieve the and_vertebrates data set\n?and_vertebrates\n\nstarting httpd help server ... done\n\n# Explore data structure\nstr(and_vertebrates)\n\ntibble [32,209 × 16] (S3: tbl_df/tbl/data.frame)\n $ year       : num [1:32209] 1987 1987 1987 1987 1987 ...\n $ sitecode   : chr [1:32209] \"MACKCC-L\" \"MACKCC-L\" \"MACKCC-L\" \"MACKCC-L\" ...\n $ section    : chr [1:32209] \"CC\" \"CC\" \"CC\" \"CC\" ...\n $ reach      : chr [1:32209] \"L\" \"L\" \"L\" \"L\" ...\n $ pass       : num [1:32209] 1 1 1 1 1 1 1 1 1 1 ...\n $ unitnum    : num [1:32209] 1 1 1 1 1 1 1 1 1 1 ...\n $ unittype   : chr [1:32209] \"R\" \"R\" \"R\" \"R\" ...\n $ vert_index : num [1:32209] 1 2 3 4 5 6 7 8 9 10 ...\n $ pitnumber  : num [1:32209] NA NA NA NA NA NA NA NA NA NA ...\n $ species    : chr [1:32209] \"Cutthroat trout\" \"Cutthroat trout\" \"Cutthroat trout\" \"Cutthroat trout\" ...\n $ length_1_mm: num [1:32209] 58 61 89 58 93 86 107 131 103 117 ...\n $ length_2_mm: num [1:32209] NA NA NA NA NA NA NA NA NA NA ...\n $ weight_g   : num [1:32209] 1.75 1.95 5.6 2.15 6.9 5.9 10.5 20.6 9.55 13 ...\n $ clip       : chr [1:32209] \"NONE\" \"NONE\" \"NONE\" \"NONE\" ...\n $ sampledate : Date[1:32209], format: \"1987-10-07\" \"1987-10-07\" ...\n $ notes      : chr [1:32209] NA NA NA NA ...\n\nand_vertebrates %&gt;% \n  glimpse() %&gt;% \n  vis_dat()\n\nRows: 32,209\nColumns: 16\n$ year        &lt;dbl&gt; 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987…\n$ sitecode    &lt;chr&gt; \"MACKCC-L\", \"MACKCC-L\", \"MACKCC-L\", \"MACKCC-L\", \"MACKCC-L\"…\n$ section     &lt;chr&gt; \"CC\", \"CC\", \"CC\", \"CC\", \"CC\", \"CC\", \"CC\", \"CC\", \"CC\", \"CC\"…\n$ reach       &lt;chr&gt; \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\"…\n$ pass        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ unitnum     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2…\n$ unittype    &lt;chr&gt; \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\"…\n$ vert_index  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1, …\n$ pitnumber   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ species     &lt;chr&gt; \"Cutthroat trout\", \"Cutthroat trout\", \"Cutthroat trout\", \"…\n$ length_1_mm &lt;dbl&gt; 58, 61, 89, 58, 93, 86, 107, 131, 103, 117, 100, 127, 99, …\n$ length_2_mm &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ weight_g    &lt;dbl&gt; 1.75, 1.95, 5.60, 2.15, 6.90, 5.90, 10.50, 20.60, 9.55, 13…\n$ clip        &lt;chr&gt; \"NONE\", \"NONE\", \"NONE\", \"NONE\", \"NONE\", \"NONE\", \"NONE\", \"N…\n$ sampledate  &lt;date&gt; 1987-10-07, 1987-10-07, 1987-10-07, 1987-10-07, 1987-10-0…\n$ notes       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…"
  },
  {
    "objectID": "labs/lab_04.html#introduction-to-the-data",
    "href": "labs/lab_04.html#introduction-to-the-data",
    "title": "Lab 4: LTER Network Data",
    "section": "",
    "text": "# Install LTER Data Sampler\n#remotes::install_github(\"lter/lterdatasampler\")\n\n# Install necessary libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3\n\nlibrary(lterdatasampler)\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.4.3\n\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.4.3\n\n\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nlibrary(visdat)\n\nWarning: package 'visdat' was built under R version 4.4.3\n\nlibrary(broom)\nlibrary(flextable)\n\n\nAttaching package: 'flextable'\n\nThe following objects are masked from 'package:ggpubr':\n\n    border, font, rotate\n\nThe following object is masked from 'package:purrr':\n\n    compose\n\n# Retrieve the and_vertebrates data set\n?and_vertebrates\n\nstarting httpd help server ... done\n\n# Explore data structure\nstr(and_vertebrates)\n\ntibble [32,209 × 16] (S3: tbl_df/tbl/data.frame)\n $ year       : num [1:32209] 1987 1987 1987 1987 1987 ...\n $ sitecode   : chr [1:32209] \"MACKCC-L\" \"MACKCC-L\" \"MACKCC-L\" \"MACKCC-L\" ...\n $ section    : chr [1:32209] \"CC\" \"CC\" \"CC\" \"CC\" ...\n $ reach      : chr [1:32209] \"L\" \"L\" \"L\" \"L\" ...\n $ pass       : num [1:32209] 1 1 1 1 1 1 1 1 1 1 ...\n $ unitnum    : num [1:32209] 1 1 1 1 1 1 1 1 1 1 ...\n $ unittype   : chr [1:32209] \"R\" \"R\" \"R\" \"R\" ...\n $ vert_index : num [1:32209] 1 2 3 4 5 6 7 8 9 10 ...\n $ pitnumber  : num [1:32209] NA NA NA NA NA NA NA NA NA NA ...\n $ species    : chr [1:32209] \"Cutthroat trout\" \"Cutthroat trout\" \"Cutthroat trout\" \"Cutthroat trout\" ...\n $ length_1_mm: num [1:32209] 58 61 89 58 93 86 107 131 103 117 ...\n $ length_2_mm: num [1:32209] NA NA NA NA NA NA NA NA NA NA ...\n $ weight_g   : num [1:32209] 1.75 1.95 5.6 2.15 6.9 5.9 10.5 20.6 9.55 13 ...\n $ clip       : chr [1:32209] \"NONE\" \"NONE\" \"NONE\" \"NONE\" ...\n $ sampledate : Date[1:32209], format: \"1987-10-07\" \"1987-10-07\" ...\n $ notes      : chr [1:32209] NA NA NA NA ...\n\nand_vertebrates %&gt;% \n  glimpse() %&gt;% \n  vis_dat()\n\nRows: 32,209\nColumns: 16\n$ year        &lt;dbl&gt; 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987…\n$ sitecode    &lt;chr&gt; \"MACKCC-L\", \"MACKCC-L\", \"MACKCC-L\", \"MACKCC-L\", \"MACKCC-L\"…\n$ section     &lt;chr&gt; \"CC\", \"CC\", \"CC\", \"CC\", \"CC\", \"CC\", \"CC\", \"CC\", \"CC\", \"CC\"…\n$ reach       &lt;chr&gt; \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\"…\n$ pass        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ unitnum     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2…\n$ unittype    &lt;chr&gt; \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\"…\n$ vert_index  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1, …\n$ pitnumber   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ species     &lt;chr&gt; \"Cutthroat trout\", \"Cutthroat trout\", \"Cutthroat trout\", \"…\n$ length_1_mm &lt;dbl&gt; 58, 61, 89, 58, 93, 86, 107, 131, 103, 117, 100, 127, 99, …\n$ length_2_mm &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ weight_g    &lt;dbl&gt; 1.75, 1.95, 5.60, 2.15, 6.90, 5.90, 10.50, 20.60, 9.55, 13…\n$ clip        &lt;chr&gt; \"NONE\", \"NONE\", \"NONE\", \"NONE\", \"NONE\", \"NONE\", \"NONE\", \"N…\n$ sampledate  &lt;date&gt; 1987-10-07, 1987-10-07, 1987-10-07, 1987-10-07, 1987-10-0…\n$ notes       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…"
  },
  {
    "objectID": "labs/lab_04.html#guided-data-analysis-part-1",
    "href": "labs/lab_04.html#guided-data-analysis-part-1",
    "title": "Lab 4: LTER Network Data",
    "section": "Guided Data Analysis Part 1",
    "text": "Guided Data Analysis Part 1\n\n## Guided Data Analysis Part 1\nand_vertebrates %&gt;% \n  filter(species == \"Cutthroat trout\") %&gt;% \n  drop_na(unittype) %&gt;% \n  count(unittype)\n\n# A tibble: 7 × 2\n  unittype     n\n  &lt;chr&gt;    &lt;int&gt;\n1 C        11419\n2 I           23\n3 IP         105\n4 P         5470\n5 R          420\n6 S            9\n7 SC        2377\n\ntrout_clean &lt;- and_vertebrates %&gt;% \n  filter(species == \"Cutthroat trout\") %&gt;%    # Filter for cutthroat\n  filter(unittype %in% c(\"C\", \"P\", \"SC\")) %&gt;%   # Filter for the 3 most abundant unittypes\n  drop_na(unittype, section)   # Drop NA values for unittype and section\n\n# Save Cutthroat trout table\ncont_table &lt;- table(trout_clean$section, trout_clean$unittype)\n\n# Conduct chi-squared test on the Cutthroat trout data\nchisq.test(cont_table)\n\n\n    Pearson's Chi-squared test\n\ndata:  cont_table\nX-squared = 188.68, df = 2, p-value &lt; 2.2e-16\n\n# Plot the Cutthroat trout data\n\n# Bar plot\ntrout_clean_barplot &lt;- trout_clean %&gt;% \n  count(unittype, section) %&gt;% \n  ggbarplot(x = 'unittype', y = 'n',\n            fill = 'section',\n            palette = c(\"#00AFBB\", \"#E7B800\"))\n\n# Box Plot\ntrout_clean_boxplot &lt;- trout_clean %&gt;% \n  drop_na(weight_g) %&gt;% \n  ggviolin(x = \"section\", y = \"weight_g\",\n           add = \"boxplot\",\n           color = \"section\",\n           palette = c(\"#00AFBB\", \"#E7B800\"))\n  \n# T-Test Assumptions\ncc_weight &lt;- trout_clean %&gt;% \n  filter(section == \"CC\") %&gt;% \n  pull(weight_g)\n\nog_weight &lt;- trout_clean %&gt;%  \n  filter(section == \"OG\") %&gt;% \n  pull(weight_g)\n\nvar.test(cc_weight, og_weight)\n\n\n    F test to compare two variances\n\ndata:  cc_weight and og_weight\nF = 1.2889, num df = 6310, denom df = 5225, p-value &lt; 2.2e-16\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 1.223686 1.357398\nsample estimates:\nratio of variances \n          1.288892 \n\n# Plot Histograms\nggarrange(gghistogram(cc_weight, main = \"Clear Cut\"), gghistogram(og_weight, main = \"Old Growth\"))\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\nWarning: Removed 4273 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3456 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n# Test log normalization\nvar.test(log(cc_weight), log(og_weight))\n\n\n    F test to compare two variances\n\ndata:  log(cc_weight) and log(og_weight)\nF = 1.0208, num df = 6310, denom df = 5225, p-value = 0.4374\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.9691443 1.0750427\nsample estimates:\nratio of variances \n          1.020787 \n\n# Default t-test with log-normalized data\nt.test(log(trout_clean$weight_g) ~ trout_clean$section, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  log(trout_clean$weight_g) by trout_clean$section\nt = 2.854, df = 11535, p-value = 0.004324\nalternative hypothesis: true difference in means between group CC and group OG is not equal to 0\n95 percent confidence interval:\n 0.02222425 0.11969560\nsample estimates:\nmean in group CC mean in group OG \n        1.457042         1.386082 \n\n# Welch Two Sample t-test\nt.test(trout_clean$weight_g ~ trout_clean$section, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  trout_clean$weight_g by trout_clean$section\nt = 4.5265, df = 11491, p-value = 6.056e-06\nalternative hypothesis: true difference in means between group CC and group OG is not equal to 0\n95 percent confidence interval:\n 0.4642016 1.1733126\nsample estimates:\nmean in group CC mean in group OG \n        8.988807         8.170050 \n\n# Coastal Giant Salamander\n# Filter for salamander data\nsally_clean &lt;- and_vertebrates %&gt;% \n  filter(species == \"Coastal giant salamander\") %&gt;% \n  drop_na(length_2_mm, weight_g)   # Remove NA values from the data.\n\n# Display salamander histograms\nggarrange(gghistogram(sally_clean$length_2_mm, title = \"Length\"),\ngghistogram(sally_clean$weight_g, title = \"Weight\"))\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\n\n\n\n\n\n\n# Test for normal distribution\ns &lt;- sally_clean  %&gt;%  \n  slice_sample(n = 5000) \n\nshapiro.test(s$length_2_mm)\n\n\n    Shapiro-Wilk normality test\n\ndata:  s$length_2_mm\nW = 0.93438, p-value &lt; 2.2e-16\n\nshapiro.test(s$weight_g)\n\n\n    Shapiro-Wilk normality test\n\ndata:  s$weight_g\nW = 0.56981, p-value &lt; 2.2e-16\n\n# Display log-normalized Salamander histograms\nggarrange(\n gghistogram(log(sally_clean$length_2_mm), title = \"Length\"), \n gghistogram(log(sally_clean$weight_g), title = \"Weight\") )\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\n\n\n\n\n\n\n# Correlation Test for Salamander Data\ncor.test(log(sally_clean$length_2_mm), log(sally_clean$weight_g))\n\n\n    Pearson's product-moment correlation\n\ndata:  log(sally_clean$length_2_mm) and log(sally_clean$weight_g)\nt = 402.85, df = 6229, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9804036 0.9822403\nsample estimates:\n      cor \n0.9813443 \n\n# Visualized correlation for Salamander Data\nsally_clean %&gt;%\n  mutate(log_length = log(length_2_mm), log_weight = log(weight_g)) %&gt;% \n  ggscatter(x = 'log_length',\n            y = 'log_weight',\n            alpha = .35,\n            add = \"loess\")\n\n\n\n\n\n\n\n# Spearman Correlation Test for Salamander Data\ncor.test(sally_clean$length_2_mm, sally_clean$weight_g, method = \"spearman\")\n\nWarning in cor.test.default(sally_clean$length_2_mm, sally_clean$weight_g, :\nCannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  sally_clean$length_2_mm and sally_clean$weight_g\nS = 819296957, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.9796802"
  },
  {
    "objectID": "labs/lab_04.html#guided-data-analysis-part-2",
    "href": "labs/lab_04.html#guided-data-analysis-part-2",
    "title": "Lab 4: LTER Network Data",
    "section": "Guided Data Analysis Part 2",
    "text": "Guided Data Analysis Part 2\n\n# Data set\ndata(\"pie_crab\")\n\n# Explore the data\nglimpse(pie_crab)\n\nRows: 392\nColumns: 9\n$ date          &lt;date&gt; 2016-07-24, 2016-07-24, 2016-07-24, 2016-07-24, 2016-07…\n$ latitude      &lt;dbl&gt; 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, …\n$ site          &lt;chr&gt; \"GTM\", \"GTM\", \"GTM\", \"GTM\", \"GTM\", \"GTM\", \"GTM\", \"GTM\", …\n$ size          &lt;dbl&gt; 12.43, 14.18, 14.52, 12.94, 12.45, 12.99, 10.32, 11.19, …\n$ air_temp      &lt;dbl&gt; 21.792, 21.792, 21.792, 21.792, 21.792, 21.792, 21.792, …\n$ air_temp_sd   &lt;dbl&gt; 6.391, 6.391, 6.391, 6.391, 6.391, 6.391, 6.391, 6.391, …\n$ water_temp    &lt;dbl&gt; 24.502, 24.502, 24.502, 24.502, 24.502, 24.502, 24.502, …\n$ water_temp_sd &lt;dbl&gt; 6.121, 6.121, 6.121, 6.121, 6.121, 6.121, 6.121, 6.121, …\n$ name          &lt;chr&gt; \"Guana Tolomoto Matanzas NERR\", \"Guana Tolomoto Matanzas…\n\nvis_dat(pie_crab)\n\n\n\n\n\n\n\n?pie_crab\n\n# sample size per site\ncount(pie_crab, site)\n\n# A tibble: 13 × 2\n   site      n\n   &lt;chr&gt; &lt;int&gt;\n 1 BC       37\n 2 CC       27\n 3 CT       33\n 4 DB       30\n 5 GTM      28\n 6 JC       30\n 7 NB       29\n 8 NIB      30\n 9 PIE      28\n10 RC       25\n11 SI       30\n12 VCR      30\n13 ZI       35\n\nsummary(pie_crab)\n\n      date               latitude         site                size      \n Min.   :2016-07-24   Min.   :30.00   Length:392         Min.   : 6.64  \n 1st Qu.:2016-07-28   1st Qu.:34.00   Class :character   1st Qu.:12.02  \n Median :2016-08-01   Median :39.10   Mode  :character   Median :14.44  \n Mean   :2016-08-02   Mean   :37.69                      Mean   :14.66  \n 3rd Qu.:2016-08-09   3rd Qu.:41.60                      3rd Qu.:17.34  \n Max.   :2016-08-13   Max.   :42.70                      Max.   :23.43  \n    air_temp      air_temp_sd      water_temp    water_temp_sd  \n Min.   :10.29   Min.   :6.391   Min.   :13.98   Min.   :4.838  \n 1st Qu.:12.05   1st Qu.:8.110   1st Qu.:14.33   1st Qu.:6.567  \n Median :13.93   Median :8.410   Median :17.50   Median :6.998  \n Mean   :15.20   Mean   :8.654   Mean   :17.65   Mean   :7.252  \n 3rd Qu.:18.63   3rd Qu.:9.483   3rd Qu.:20.54   3rd Qu.:7.865  \n Max.   :21.79   Max.   :9.965   Max.   :24.50   Max.   :9.121  \n     name          \n Length:392        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n# ANOVA\npie_crab |&gt; \n  ggboxplot(x = 'site', y = 'size', col = 'site') +\n  geom_jitter(size =.25) + \n  theme(legend.postition = \"none\")\n\nWarning in plot_theme(plot): The `legend.postition` theme element is not\ndefined in the element hierarchy.\n\n\n\n\n\n\n\n\n# Assumptions \n# Normality\nnorms &lt;- pie_crab |&gt; \n  nest(data = -site) |&gt;\n  mutate(Shapiro = map(data, ~ shapiro.test(.x$size)),\n         n = map_dbl(data, nrow),\n         glance_shapiro = map(Shapiro, broom::glance)) |&gt;\n  unnest(glance_shapiro)\n\nflextable::flextable(dplyr::select(norms, site, n, statistic, p.value)) |&gt;\n  flextable::set_caption(\"Shapiro-Wilk normality test for size at each site\")\n\nsitenstatisticp.valueGTM280.90078140.0119337484SI300.97053520.5539208550NIB300.97282970.6191340731ZI350.97445830.5765589900RC250.93150620.0941588802VCR300.94446820.1200262239DB300.95762710.2690631942JC300.96347540.3788327942CT330.92773650.0301785639NB290.96753670.4949587443CC270.93546590.0941803007BC370.88857210.0014402753PIE280.84893990.0008899392\n\n# Residuals\n(res_aov &lt;- aov(size ~ site, data = pie_crab))\n\nCall:\n   aov(formula = size ~ site, data = pie_crab)\n\nTerms:\n                    site Residuals\nSum of Squares  2172.376  2626.421\nDeg. of Freedom       12       379\n\nResidual standard error: 2.632465\nEstimated effects may be unbalanced\n\ngghistogram(res_aov$residuals)\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\n\n\n\n\n\n\nshapiro.test(res_aov$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  res_aov$residuals\nW = 0.99708, p-value = 0.7122\n\n# Equal Variances\nleveneTest(size ~ site, data = pie_crab)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup  12  9.2268 1.151e-15 ***\n      379                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# ANOVA Tests\n# Welch's ANOVA\noneway.test(size ~ site, data = pie_crab, var.equal = FALSE)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  size and site\nF = 39.108, num df = 12.00, denom df = 145.79, p-value &lt; 2.2e-16\n\n# Filter a subset of the sites\npie_sites &lt;- pie_crab |&gt; \n  filter(site %in% c(\"GTM\", \"DB\", \"PIE\"))\n\n# Check for equal variance\nleveneTest(size ~ site, data = pie_sites)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2   0.548 0.5802\n      83               \n\n# Note that the variances are equal (p = 0.5802), so we can proceed with the ANOVA\n\n# ANOVA for the data subset\npie_anova &lt;- aov(size ~ site, data = pie_sites)\n\n# View the ANOVA results \nsummary(pie_anova)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nsite         2  521.5  260.75   60.02 &lt;2e-16 ***\nResiduals   83  360.6    4.34                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Post-hoc Tukey's HSD Test\nTukeyHSD(pie_anova)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = size ~ site, data = pie_sites)\n\n$site\n             diff       lwr       upr   p adj\nGTM-DB  -3.200786 -4.507850 -1.893722 3.0e-07\nPIE-DB   2.899929  1.592865  4.206992 2.9e-06\nPIE-GTM  6.100714  4.771306  7.430123 0.0e+00\n\n# Linear Regression\npie_lm &lt;- lm(size ~ latitude, data = pie_crab)\n\n# View the results of the linear model\nsummary(pie_lm)\n\n\nCall:\nlm(formula = size ~ latitude, data = pie_crab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8376 -1.8797  0.1144  1.9484  6.9280 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.62442    1.27405  -2.845  0.00468 ** \nlatitude     0.48512    0.03359  14.441  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.832 on 390 degrees of freedom\nMultiple R-squared:  0.3484,    Adjusted R-squared:  0.3467 \nF-statistic: 208.5 on 1 and 390 DF,  p-value: &lt; 2.2e-16\n\n# Linear regression visualized\npie_crab |&gt; \n  ggscatter(x = 'latitude', y = 'size', \n            alpha = .35, \n            add = \"reg.line\")\n\n\n\n\n\n\n\n# Predictions from linear regression\nnew_lat &lt;- data.frame(latitude = c(32, 36, 38))\nbroom::augment(pie_lm, newdata = new_lat)\n\n# A tibble: 3 × 2\n  latitude .fitted\n     &lt;dbl&gt;   &lt;dbl&gt;\n1       32    11.9\n2       36    13.8\n3       38    14.8\n\n# Multiple linear regression\npie_mlm &lt;- lm(size ~ latitude + air_temp + water_temp, data = pie_crab)\n\nsummary(pie_mlm)\n\n\nCall:\nlm(formula = size ~ latitude + air_temp + water_temp, data = pie_crab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7099 -1.7195 -0.0602  1.7823  7.7271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  77.7460    17.3477   4.482 9.76e-06 ***\nlatitude     -1.0587     0.3174  -3.336 0.000933 ***\nair_temp     -2.4041     0.3844  -6.255 1.05e-09 ***\nwater_temp    0.7563     0.1465   5.162 3.92e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.677 on 388 degrees of freedom\nMultiple R-squared:  0.4206,    Adjusted R-squared:  0.4161 \nF-statistic:  93.9 on 3 and 388 DF,  p-value: &lt; 2.2e-16\n\n# Test mlm for correlations\npie_crab |&gt; \n  select(latitude, air_temp, water_temp) |&gt; \n  cor()\n\n             latitude   air_temp water_temp\nlatitude    1.0000000 -0.9949715 -0.9571738\nair_temp   -0.9949715  1.0000000  0.9632287\nwater_temp -0.9571738  0.9632287  1.0000000"
  },
  {
    "objectID": "labs/lab_04.html#part-2-lab-exercise",
    "href": "labs/lab_04.html#part-2-lab-exercise",
    "title": "Lab 4: LTER Network Data",
    "section": "Part 2 Lab Exercise",
    "text": "Part 2 Lab Exercise\n\n## Prework for Question 2-1\npie_crab %&gt;% \n  ggboxplot(x = 'site', y = 'size', col = 'site') +\n  geom_jitter(size =.25) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Shapiro-Wilk Normality Test\nnorms &lt;- pie_crab %&gt;%  \n  nest(data = -site) %&gt;% \n  mutate(Shapiro = map(data, ~ shapiro.test(.x$size)),\n         n = map_dbl(data, nrow),\n         glance_shapiro = map(Shapiro, broom::glance)) %&gt;%\n  unnest(glance_shapiro)\n\nflextable(select(norms, site, n, statistic, p.value)) %&gt;% \n  set_caption(\"Shapiro-Wilk Normality Test for Size at Each Site\")\n\nsitenstatisticp.valueGTM280.90078140.0119337484SI300.97053520.5539208550NIB300.97282970.6191340731ZI350.97445830.5765589900RC250.93150620.0941588802VCR300.94446820.1200262239DB300.95762710.2690631942JC300.96347540.3788327942CT330.92773650.0301785639NB290.96753670.4949587443CC270.93546590.0941803007BC370.88857210.0014402753PIE280.84893990.0008899392\n\n# Normal distribution check\n(res_aov &lt;- aov(size ~ site, data = pie_crab))\n\nCall:\n   aov(formula = size ~ site, data = pie_crab)\n\nTerms:\n                    site Residuals\nSum of Squares  2172.376  2626.421\nDeg. of Freedom       12       379\n\nResidual standard error: 2.632465\nEstimated effects may be unbalanced\n\ngghistogram(res_aov$residuals)\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\n\n\n\n\n\n\nshapiro.test(res_aov$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  res_aov$residuals\nW = 0.99708, p-value = 0.7122\n\n# Levene's Test\nleveneTest(size ~ site, data = pie_crab)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup  12  9.2268 1.151e-15 ***\n      379                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Welch's ANOVA\noneway.test(size ~ site, data = pie_crab, var.equal = FALSE)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  size and site\nF = 39.108, num df = 12.00, denom df = 145.79, p-value &lt; 2.2e-16\n\n# Filter a subset of the sites\npie_sites &lt;- pie_crab |&gt; \n  filter(site %in% c(\"GTM\", \"DB\", \"PIE\"))\n\n# Check for equal variance\nleveneTest(size ~ site, data = pie_sites)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2   0.548 0.5802\n      83               \n\n# Note that the variances are equal (p = 0.5802), so we can proceed with the ANOVA\n\n# ANOVA for the data subset\npie_anova &lt;- aov(size ~ site, data = pie_sites)\n\n# View the ANOVA results \nsummary(pie_anova)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nsite         2  521.5  260.75   60.02 &lt;2e-16 ***\nResiduals   83  360.6    4.34                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Post-hoc Tukey's HSD Test\nTukeyHSD(pie_anova)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = size ~ site, data = pie_sites)\n\n$site\n             diff       lwr       upr   p adj\nGTM-DB  -3.200786 -4.507850 -1.893722 3.0e-07\nPIE-DB   2.899929  1.592865  4.206992 2.9e-06\nPIE-GTM  6.100714  4.771306  7.430123 0.0e+00\n\n## Question 2-1\n# Organize by latitude\npie_crab &lt;- pie_crab %&gt;% \n  arrange(latitude) %&gt;% \n  mutate(site = factor(site, levels = unique(site)))\n\n# Plot data\nbox_plot_crab_sites &lt;- ggplot(pie_crab, aes(x = site, y = size)) +\n  geom_boxplot(fill = \"orange\", color = \"black\") +\n  geom_jitter(size =.25) +\n  labs(title = \"Carapace Width by Site, Ordered by Latitude\",\n       x = \"Site\",\n       y = \"Carapace Width (mm)\") +\n  theme_minimal()   \nprint(box_plot_crab_sites)\n\n\n\n\n\n\n\n#ggsave(\"box_plot_crab_sites.png\", plot = box_plot_crab_sites, width = 10, height = 6, dpi = 300)\n\n## Question 2-2\n# Linear regression model\npie_lm_water &lt;- lm(size ~ water_temp_sd, data = pie_crab)\nsummary(pie_lm_water)\n\n\nCall:\nlm(formula = size ~ water_temp_sd, data = pie_crab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.9428 -2.6948 -0.2145  2.6573  8.8070 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   13.93728    1.15338  12.084   &lt;2e-16 ***\nwater_temp_sd  0.09938    0.15716   0.632    0.528    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.506 on 390 degrees of freedom\nMultiple R-squared:  0.001024,  Adjusted R-squared:  -0.001537 \nF-statistic: 0.3999 on 1 and 390 DF,  p-value: 0.5275\n\n# Linear Regression Line of Best fit\nLOBF &lt;- pie_crab %&gt;% \n  ggscatter(x = 'water_temp_sd', y = 'size',\n  alpha = .35,\n  add = \"reg.line\")\nprint(LOBF)\n\n\n\n\n\n\n\n#ggsave(\"LOBF.png\", plot = LOBF, width = 10, height = 6, dpi = 300)\n\n## Question 2-3\n# Check for correlations\npie_crab %&gt;% \n  dplyr::select(latitude, air_temp_sd, water_temp_sd) %&gt;% \n  cor()\n\n                latitude air_temp_sd water_temp_sd\nlatitude      1.00000000   0.7932130    0.04188273\nair_temp_sd   0.79321301   1.0000000    0.40970338\nwater_temp_sd 0.04188273   0.4097034    1.00000000\n\n# Multiple linear regression model\ncrab_mlm &lt;- lm(size ~ latitude + air_temp_sd + water_temp_sd, data = pie_crab)\nsummary(crab_mlm)\n\n\nCall:\nlm(formula = size ~ latitude + air_temp_sd + water_temp_sd, data = pie_crab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7515 -1.8897  0.0506  1.9301  6.6746 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -3.96880    1.54818  -2.564   0.0107 *  \nlatitude       0.55940    0.06413   8.723   &lt;2e-16 ***\nair_temp_sd   -0.41713    0.30559  -1.365   0.1730    \nwater_temp_sd  0.15927    0.16174   0.985   0.3254    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.832 on 388 degrees of freedom\nMultiple R-squared:  0.3516,    Adjusted R-squared:  0.3466 \nF-statistic: 70.13 on 3 and 388 DF,  p-value: &lt; 2.2e-16\n\n\n\nQuestion 2-1:\nThe results of the ANOVA test are highly significant. At least one site has significantly different carapace widths than the other sites. We know this because the p-value is less than 0.05 (p &lt; 2.2e-16), leading us to reject the null hypothesis. The Tukey’s HSD Test shows us that the site relationships: GTM-DB, PIE-DB, and PIE-GTM are statistically significant, because their p-values are all less than 0.05. This represents that each of these related sites are significantly different from each other.\n\n\nQuestion 2-2:\nThe p-value is greater than 0.05, we fail to reject the null hypothesis. There is not enough statistical evidence to represent a significant relationship between water_temp_sd and crab carapace width.\n\n\nQuestion 2-3:\nair_temp_sd and latitude have a higher correlation coefficient, with it being greater than 0.7 (0.79321301), but water_temp_sd and air_temp_sd, and latitude and water_temp_sd, are not highly correlated. Highly correlated variables increase the complexity of the linear regression model. The overall p-value is less than 0.05 (p &lt; 2.2e-16), suggesting a significant impact from the combination of predictors on crab carapace width. The individual p-values vary. Latitude is the only one less than 0.05, meaning it is the only statistically significant relationship with an effect on crab size. air_temp_sd and water_temp_sd both have p-values greater than 0.05, failing to reject the null hypothesis. There is not enough statistical evidence to represent a statistically significant relationship between these two predictors and crab carapace width."
  },
  {
    "objectID": "labs/lab_02.html",
    "href": "labs/lab_02.html",
    "title": "Lab 2: Minnesota Tree Growth",
    "section": "",
    "text": "library(quarto)\n\nWarning: package 'quarto' was built under R version 4.4.3\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.4.3\n\n\nhere() starts at C:/Users/Zacha/github/CSU/csu_ess_330\n\n# Data Import\ntree_dat &lt;- read.csv(here(\"data\", \"lab_data\", \"tree_dat.csv\"))\n\n\n# Question 1: Read in the Minnesota tree growth dataset. Use glimpse to understand the structure and names of the dataset. Decribe the structure and what you see in the dataset?\nglimpse(tree_dat)\n\nRows: 131,386\nColumns: 8\n$ treeID  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ standID &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ stand   &lt;chr&gt; \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A…\n$ year    &lt;int&gt; 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 19…\n$ species &lt;chr&gt; \"ABBA\", \"ABBA\", \"ABBA\", \"ABBA\", \"ABBA\", \"ABBA\", \"ABBA\", \"ABBA\"…\n$ age     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ inc     &lt;dbl&gt; 0.930, 0.950, 0.985, 0.985, 0.715, 0.840, 0.685, 0.940, 1.165,…\n$ rad_ib  &lt;dbl&gt; 10.78145, 11.73145, 12.71645, 13.70145, 14.41645, 15.25645, 15…\n\n\nQuestion 1: This data table has 8 columns; treeID, standID, stand, year, species, age, inc, rad_ib. There are 131,386 rows.\n\n# Question 2: How many records have been made in stand 1?\ntree_dat %&gt;%\n    filter(standID == 1) %&gt;%\n    tally() # Find the number of recorded data from stand 1.\n\n    n\n1 979\n\n\nQuestion 2: 979 records have been made in stand 1.\n\n# Question 3: How many records of the Abies balsamea and Pinus strobus species have been made?\ntree_dat %&gt;%\n    filter(species %in% c(\"ABBA\", \"PIST\")) %&gt;%\n    count(species) # Find the number of times two species have been recorded.\n\n  species     n\n1    ABBA 13033\n2    PIST  4188\n\n\nQuestion 3: There are 13,033 records of Abies balsamea and 4188 records of Pinus Strobus.\n\n# Question 4: How many trees are older then 200 years old in the last year of the dataset?\nlast_year &lt;- max(tree_dat$year, na.rm = TRUE) # Find the last year of the data set.\n\ntree_dat %&gt;%\n  filter(year == last_year, age &gt; 200) %&gt;%\n  tally() # Counts trees older than 200 years in the last year.\n\n  n\n1 7\n\n\nQuestion 4: In the last year recorded, there were 7 trees older than 200.\n\n# Question 5: What is the oldest tree in the dataset found using slice_max?\ntree_dat %&gt;%\n  slice_max(order_by = age, n = 1)\n\n  treeID standID stand year species age  inc rad_ib\n1     24       2    A2 2007    PIRE 269 0.37 308.84\n\n\nQuestion 5: The oldest tree in the data set is treeID 24, a Pinus resinosa at 269 yoa.\n\n# Question 6: Find the oldest 5 trees recorded in 2001. Use the help docs to understand optional parameters\ntree_dat %&gt;%\n  filter(year == 2001) %&gt;%\n  slice_max(order_by = age, n = 5)\n\n  treeID standID stand year species age   inc  rad_ib\n1     24       2    A2 2001    PIRE 263 0.210 306.880\n2     25       2    A2 2001    PIRE 259 0.280 156.210\n3   1595      24    F1 2001    FRNI 212 0.579 156.267\n4   1598      24    F1 2001    FRNI 206 0.394 130.251\n5   1712      26    F3 2001    FRNI 206 0.168 154.354\n\n\nQuestion 6: The oldest five trees in measured in 2001 include three Fraxinus nigra (FRNI) individuals and two Pinus resinosa (PIRE) individuals. Oldeds to youngest: 263 (PIRE), 259 (PIRE), 212 (FRNI), 206 (FRNI), 206 (FRNI).\n\n# Question 7: Using slice_sample, how many trees are in a 30% sample of those recorded in 2002?\ntree_dat %&gt;%\n  filter(year == 2002) %&gt;%\n  slice_sample(prop = 0.3) %&gt;%\n  tally ()\n\n    n\n1 687\n\n\nQuestion 7: There are 687 trees recorded in a 30% sample of 2002.\n\n#Question 8: Filter all trees in stand 5 in 2007. Sort this subset by descending radius at breast height (rad_ib) and use slice_head() to get the top three trees. Report the tree IDs.\ntree_dat %&gt;%\n  filter(year == 2007, standID == 5) %&gt;%\n  arrange(desc(rad_ib)) %&gt;%\n  slice_head(n = 3)\n\n  treeID standID stand year species age   inc   rad_ib\n1    128       5    A6 2007    PIST  82 0.885 238.8850\n2    157       5    A6 2007    PIRE  85 0.900 217.8700\n3    135       5    A6 2007    PIMA  84 0.110 210.1874\n\n\nQuestion 8: The three largest trees in stand 5 when measured in 2007 were (in descending order) treeIDs: 128, 157, 135.\n\n# Question 9: Reduce your full data.frame to [treeID, stand, year, and radius at breast height]. Filter to only those in stand 3 with records from 2007, and use slice_min to pull the smallest three trees measured that year.\ntree_dat %&gt;%\n  select(\"treeID\", \"standID\", \"year\", \"rad_ib\") %&gt;%\n  filter(standID == 3, year == 2007) %&gt;%\n  slice_min(order_by = rad_ib, n = 3)\n\n  treeID standID year rad_ib\n1     50       3 2007 47.396\n2     56       3 2007 48.440\n3     36       3 2007 54.925\n\n\nQuestion 9: The three smallest trees measured in stand 3 during 2007 were (from smallest to largest) tree ID: 50, 56, 36. The range of the three smallest trees was 47.396 to 54.925 mm.\n\n# Question 10: Use select to remove the stand column. Use glimspe to show the dataset.\ntree_dat %&gt;% \n  select(-\"stand\") %&gt;% \n  glimpse()\n\nRows: 131,386\nColumns: 7\n$ treeID  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ standID &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ year    &lt;int&gt; 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 19…\n$ species &lt;chr&gt; \"ABBA\", \"ABBA\", \"ABBA\", \"ABBA\", \"ABBA\", \"ABBA\", \"ABBA\", \"ABBA\"…\n$ age     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ inc     &lt;dbl&gt; 0.930, 0.950, 0.985, 0.985, 0.715, 0.840, 0.685, 0.940, 1.165,…\n$ rad_ib  &lt;dbl&gt; 10.78145, 11.73145, 12.71645, 13.70145, 14.41645, 15.25645, 15…\n\n\n\n# Question 11: Look at the help document for dplyr::select and examine the “Overview of selection features”. Identify an option (there are multiple) that would help select all columns with the string “ID” in the name. Using glimpse to view the remaining data set.\ntree_dat %&gt;% \n  select(contains(\"ID\")) %&gt;% # Could also use \"ends_with()\" or \"matches()\", though the former is limited in application requiring the column headers to end in \"ID\".\n  glimpse()\n\nRows: 131,386\nColumns: 2\n$ treeID  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ standID &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\n\n\n# Question 12: Find a selection pattern that captures all columns with either ‘ID’ or ‘stand’ in the name. Use glimpse to verify the selection.\ntree_dat %&gt;% \n  select(contains(\"ID\")|contains(\"stand\")) %&gt;% \n  glimpse()\n\nRows: 131,386\nColumns: 3\n$ treeID  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ standID &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ stand   &lt;chr&gt; \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A…\n\n\n\n# Question 13: Looking back at the data dictionary, rename rad_ib and inc to include _[unit] in the name. Unlike earlier options, be sure that this renaming is permanent, and stays with your data.frame (e.g. &lt;-). Use glimpse to view your new data.frame.\ntree_dat_mm &lt;- tree_dat %&gt;% \n  rename(\"rad_ib_mm\" = \"rad_ib\", \"inc_mm\" = \"inc\")\nglimpse(tree_dat_mm)\n\nRows: 131,386\nColumns: 8\n$ treeID    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ standID   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ stand     &lt;chr&gt; \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", \"A1\", …\n$ year      &lt;int&gt; 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, …\n$ species   &lt;chr&gt; \"ABBA\", \"ABBA\", \"ABBA\", \"ABBA\", \"ABBA\", \"ABBA\", \"ABBA\", \"ABB…\n$ age       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ inc_mm    &lt;dbl&gt; 0.930, 0.950, 0.985, 0.985, 0.715, 0.840, 0.685, 0.940, 1.16…\n$ rad_ib_mm &lt;dbl&gt; 10.78145, 11.73145, 12.71645, 13.70145, 14.41645, 15.25645, …\n\n\n\n# Question 14: A key measurement in forestry in “basal area column”. The metric is computed with the formula:\n  # BA(m2) = 0.00007854⋅DBH^\n# Where DBH is the diameter at breast height (cm). Use mutate to compute DBH in centimeters, and BA in m2 (HINT: Make sure rad_ib is in cm prior to computing the diameter!). What is the mean BA_m2 of the the species POTR in 2007?\n\n# Create columns for dia. at breast height (cm) and basal area (m^2).\ntree_dat_exp &lt;- tree_dat_mm %&gt;% \n  mutate(DBH_cm = 2*rad_ib_mm/10, BA_m2 = 0.00007854*(DBH_cm)^2)\n\n# Filter for year 2007 and species POTR.\ntree_dat_exp %&gt;%   \n  filter(species == \"POTR\", year == 2007) %&gt;% \n  select(BA_m2) %&gt;% \n  pull(BA_m2) %&gt;% \n  mean()           # Find mean BA (m^2).\n\n[1] 0.03696619\n\n\nQuestion 14: The mean basal area (m^2) of Populus tremuloides (POTR) from 2007 is 0.0369 m^2.\n\n# Question 15: Lets say for the sake of our study, trees are not established until they are 5 years of age. Use if_else to add a boolean column to our dataset called established that is TRUE if the age is greater then 5 and FALSE if less then or equal to five. Once added, use count (see ?count) to determine how many records are from estabilshed trees?\ntree_dat_exp %&gt;% \n  mutate(established = if_else(age &gt;5, TRUE, FALSE)) %&gt;% \n  filter(established == TRUE) %&gt;% \n  count(TRUE)\n\n  TRUE      n\n1 TRUE 122503\n\n\nQuestion 15: There are 122503 records from established trees.\n\n# Question 16: Use mutate and case_when to add a new column to you data.frame that classifies each tree into the proper DBH_class. Once done, limit your dataset to the year 2007 and report the number of each class with count.\n\n# Adds column tree_dat_exp classifying trees based on DBH_cm.\ntree_dat_exp &lt;- tree_dat_exp %&gt;% \n  mutate(DBH_class = case_when(\n    DBH_cm &gt; 0 & DBH_cm &lt;= 2.5 ~ \"seedling\",\n    DBH_cm &gt; 2.5 & DBH_cm &lt;= 10 ~ \"sapling\",\n    DBH_cm &gt; 10 & DBH_cm &lt;= 30 ~ \"pole\",\n    DBH_cm &gt; 30 ~ \"sawlog\"\n  ))\n\n# Counts trees by DBH class for 2007.\ntree_dat_exp %&gt;% \n  filter(year == 2007) %&gt;%\n  count(DBH_class) %&gt;% \n  complete(DBH_class = c(\"seedling\", \"sapling\", \"pole\", \"sawlog\"), fill = list(n=0))\n\n# A tibble: 4 × 2\n  DBH_class     n\n  &lt;chr&gt;     &lt;int&gt;\n1 pole       1963\n2 sapling     252\n3 sawlog       76\n4 seedling      0\n\n\nQuestion 16: In 2007 there were 0 seedlings, 252 saplings, 1963 poles and 76 sawlogs recorded.\n\n# Question 17: Compute the mean DBH (in cm) and standard deviation of DBH (in cm) for all trees in 2007. Explain the values you found and their statistical meaning.\ntree_dat_exp %&gt;% \n  filter(year == 2007) %&gt;% \n  summarize(\n    mean(DBH_cm),  # Calculate mean of DBH_cm\n    sd(DBH_cm)     # Calculate stdev\n  )\n\n  mean(DBH_cm) sd(DBH_cm)\n1     16.09351   6.138643\n\n\nQuestion 17: The mean diameter at breast height was 16.094 cm and the mean standard deviation of diameter at breast height was 6.139. This means that the average diameter at breast height of all records form 2007 was ~16 cm and that 50% of the records are +/- ~6.1 cm of the average.\n\n# Question 18: Compute the per species mean tree age using only those ages recorded in 2003. Identify the three species with the oldest mean age.\n\n# Filter tree data for the year 2003\ntree_dat_2003 &lt;- tree_dat %&gt;% \n  filter(year == 2003)\n  # Compute mean age per species\n  mean_species_age_2003 &lt;- tree_dat_2003 %&gt;% \n    group_by(species) %&gt;% \n    summarize(mean_age_2003 = mean(age, na.rm = TRUE)) # Calculate mean age excluding null values.\n\n# Sort by mean age in descending order and identify the three species with the oldest (highest) means.\noldest_3_species_2003 &lt;- mean_species_age_2003 %&gt;% \n    arrange(desc(mean_age_2003)) %&gt;%\n    head(3)\n\n# Print results\nprint(\"Mean Age by Species:\")\n\n[1] \"Mean Age by Species:\"\n\nmean_species_age_2003\n\n# A tibble: 15 × 2\n   species mean_age_2003\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 ABBA             32.5\n 2 ACRU             61.7\n 3 ACSA             70.2\n 4 BEPA             72.8\n 5 FRNI             83.1\n 6 LALA             71.5\n 7 PIBA             38.5\n 8 PIGL             42.3\n 9 PIMA             64.1\n10 PIRE             51.3\n11 PIST             73.3\n12 POGR             73.0\n13 POTR             48.8\n14 QURU             64.0\n15 THOC            127. \n\nprint(\"Oldest Species by Mean Age:\")\n\n[1] \"Oldest Species by Mean Age:\"\n\noldest_3_species_2003\n\n# A tibble: 3 × 2\n  species mean_age_2003\n  &lt;chr&gt;           &lt;dbl&gt;\n1 THOC            127. \n2 FRNI             83.1\n3 PIST             73.3\n\n\nQuestion 18: The three oldest species are Thuja occidentalis, Fraxinus nigra and Pinus strobus with mean ages of 126.6, 83.1 and 73.3 respectively.\n\n#Question 19: In a single summarize call, find the number of unique years with records in the data set along with the first and last year recorded?\ntree_dat %&gt;% \n  summarize(\n    unique_years = n_distinct(year),       # Count of unique years\n    first_year = min(year, na.rm = TRUE),  # First year recorded\n    last_year = max(year, na.rm = TRUE)    # Last year recorded\n  )\n\n  unique_years first_year last_year\n1          111       1897      2007\n\n\nQuesiton 19: There are 111 unique years with records starting in 1897 and ending in 2007.\n\n# Question 20: Determine the stands with the largest number of unique years recorded. Report all stands with largest (or tied with the largest) temporal record.\n\n# Identify the number of unique years per stand\nstand_record_counts &lt;- tree_dat %&gt;%\n  group_by(stand) %&gt;% \n  summarize(unique_years = n_distinct(year, na.rm = TRUE)) # Count unique years per stand\n\n# Find the max number of unique years\nmax_unique_years &lt;- max(stand_record_counts$unique_years, na.rm = TRUE)\n\n# Filter for stands with the max unique year counts\nstands_with_max_years &lt;- stand_record_counts %&gt;% \n  filter(unique_years == max_unique_years)\n\n# Print Result\nprint(\"Stands With Max Year Counts:\")\n\n[1] \"Stands With Max Year Counts:\"\n\nstands_with_max_years\n\n# A tibble: 5 × 2\n  stand unique_years\n  &lt;chr&gt;        &lt;int&gt;\n1 A1             111\n2 D1             111\n3 D2             111\n4 D3             111\n5 F1             111\n\n\nQuestion 20: The five following stands have data from all 111 years: A1, D1, D2, D2, F1.\n\n# Final Question: Use a combination of dplyr verbs to compute these values and report the 3 species with the fastest growth, and the 3 species with the slowest growth. (** You will need to use either lag() or diff() in your compuation. You can learn more about each in the Help pages)\n\n# Calculate annual growth rate\ntree_growth &lt;- tree_dat_exp %&gt;% \n  arrange(species, treeID, year) %&gt;%               # Set correct ordering\n  group_by(species, treeID) %&gt;% \n  mutate(growth_rate = DBH_cm - lag(DBH_cm)) %&gt;%   # Caclulate growth rate\n  ungroup()\n\n# Compute mean growth rate per species\nspecies_growth &lt;- tree_growth %&gt;% \n  group_by(species) %&gt;% \n  summarize(mean_growth_rate = mean(growth_rate, na.rm = TRUE))\n\n# Identify the 3 fastest/slowest growing species\nfastest_species &lt;- species_growth %&gt;%    # 3 fastest growing species\n  arrange(desc(mean_growth_rate)) %&gt;% \n  head(3)\n\nslowest_species &lt;- species_growth %&gt;%    # 3 slowest growing species\n  arrange(mean_growth_rate) %&gt;% \n  head(3)\n\n# Print results\nprint(\"Fastest Growing SPecies:\")\n\n[1] \"Fastest Growing SPecies:\"\n\nfastest_species\n\n# A tibble: 3 × 2\n  species mean_growth_rate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 PIRE               0.358\n2 POTR               0.331\n3 PIBA               0.326\n\nprint(\"Slowest Growing Species:\")\n\n[1] \"Slowest Growing Species:\"\n\nslowest_species\n\n# A tibble: 3 × 2\n  species mean_growth_rate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 LALA               0.150\n2 THOC               0.153\n3 QURU               0.168\n\n\nQuestion 21: The three fastest growing species are Pinus resinosa, Populus tremuloides and Pinus banksiana. The three slowest growing species are Larix laricina, Thuja occidentalis, Quercus rubra. The fastest growth rate was ~0.358 cm per year at breast height. The slowest growth rate was ~0.150 cm per year at breast height."
  },
  {
    "objectID": "exercise_summary.html",
    "href": "exercise_summary.html",
    "title": "Homework Overview",
    "section": "",
    "text": "This page links to each daily exercise completed in the course. These exercises were assignments given roughly bi-weekly due before class each day.\n\n\n\nExercise 01\n\nIn this assignment we set up RStudio and practiced installing packages. No deliverable is available.\n\nExercise 02\n\nIn this assignment we set up git and GitHub, linking RStudio to GitHub. No deliverable is available.\n\nExercise 03\n\nIn this assignment we created our first GitHub repo, hello-world. This repo has now been privately archived.\n\nExercise 04\n\nIn this assignment we practiced forking other users repos. It is now publicly archived, the repo is linked.\n\nExercise 05\n\nIn this assginment we explored the ‘palmerspenguins’ package as an exercise in data manipulation in RStudio.\n\nExercise 06\n\nIn this assignment we practiced data manipulation using the public COVID-19 case data provided by the New York Times.\n\nExercise 07\n\nIn this assignment we practiced data visualization using the NY-Times COVID-19 data. This assignment was initially created and submitted as an R script and has been converted to a quarto document for the purposes of sharing documentation.\n\nExercise 08\n\nIn this assignment the NY-Times COVID-19 data was used to practice pivoting and joining data. This assignment was initially created and submitted as an R script and has been converted to a quarto document for the purposes of sharing documentation.\n\nExercise 09 & 10\n\nThis assignment was not completed as two of the four daily assignments eligable to be dropped. No deliverable is available.\n\nExercise 11 & 12\n\nIn this assignment we practiced exploratory data analysis (EDA) and basic modeling with linear regressions using the ‘airquality’ dataset.\n\nExercise 13\n\nThis assignment was a check-in completed on Canvas. No deliverable is available for this assignment.\n\nExercise 14\n\nIn this assignment we explore open source data resources. No deliverable is available for this assignment, a list of potential data sources for the course project was submitted to canvas.\n\nExercise 15\n\nIn this assignment we practiced data splitting, seeding and ml model prep with the ‘penguins’ dataset. This assignment was initially created and submitted as an R script and has been converted to a quarto document for the purposes of sharing documentation.\n\nExercise 16\n\nIn this assignment we practiced ml model building and ml workflow building based on the work completed in exercise 15 with the ‘penguins’ dataset. This assignment was initially created and submitted as an R script and has been converted to a quarto document for the purposes of sharing documentation.\n\nExercise 17\n\nThis assignment was a reading and brief concept summary completed on Canvas. No deliverable is available for this assignment.\n\nExercise 18 & 19 (This link is under construction)\n\nIn this assignment we completed modeling tasks with the NY-Times COVID-19 and census data. Exercise 19 invovled tuning the basic model. This assignment was initially created and submitted as an R script and has been converted to a quarto document for the purposes of sharing documentation.\n\nExercise 20\n\nThis assignment was replaced by in-class attendance for extra credit. There is no deliverable for this assignment.\n\nExercise 21\n\nIn this assignment we practiced visualizing time series data using USGS data from the Cache la Poudre watershed.\n\nExercise 22\n\nIn this assignment we practiced forcast modeling with time series data using USGS data from the Cache la Poudre watershed.\n\nExercise 23\n\nIn this assignment we setup geospatial packages in R and learned about some common tools, data sources and packages using R for GIS. This assignment was initially created and submitted to Canvas and has been converted to a quarto document for the purposes of sharing documentation.\n\nExercise 24\n\nIn this assignment we practiced working with Lairmer County geospatial data."
  },
  {
    "objectID": "exercise_summary.html#exercise-links",
    "href": "exercise_summary.html#exercise-links",
    "title": "Homework Overview",
    "section": "",
    "text": "Exercise 01\n\nIn this assignment we set up RStudio and practiced installing packages. No deliverable is available.\n\nExercise 02\n\nIn this assignment we set up git and GitHub, linking RStudio to GitHub. No deliverable is available.\n\nExercise 03\n\nIn this assignment we created our first GitHub repo, hello-world. This repo has now been privately archived.\n\nExercise 04\n\nIn this assignment we practiced forking other users repos. It is now publicly archived, the repo is linked.\n\nExercise 05\n\nIn this assginment we explored the ‘palmerspenguins’ package as an exercise in data manipulation in RStudio.\n\nExercise 06\n\nIn this assignment we practiced data manipulation using the public COVID-19 case data provided by the New York Times.\n\nExercise 07\n\nIn this assignment we practiced data visualization using the NY-Times COVID-19 data. This assignment was initially created and submitted as an R script and has been converted to a quarto document for the purposes of sharing documentation.\n\nExercise 08\n\nIn this assignment the NY-Times COVID-19 data was used to practice pivoting and joining data. This assignment was initially created and submitted as an R script and has been converted to a quarto document for the purposes of sharing documentation.\n\nExercise 09 & 10\n\nThis assignment was not completed as two of the four daily assignments eligable to be dropped. No deliverable is available.\n\nExercise 11 & 12\n\nIn this assignment we practiced exploratory data analysis (EDA) and basic modeling with linear regressions using the ‘airquality’ dataset.\n\nExercise 13\n\nThis assignment was a check-in completed on Canvas. No deliverable is available for this assignment.\n\nExercise 14\n\nIn this assignment we explore open source data resources. No deliverable is available for this assignment, a list of potential data sources for the course project was submitted to canvas.\n\nExercise 15\n\nIn this assignment we practiced data splitting, seeding and ml model prep with the ‘penguins’ dataset. This assignment was initially created and submitted as an R script and has been converted to a quarto document for the purposes of sharing documentation.\n\nExercise 16\n\nIn this assignment we practiced ml model building and ml workflow building based on the work completed in exercise 15 with the ‘penguins’ dataset. This assignment was initially created and submitted as an R script and has been converted to a quarto document for the purposes of sharing documentation.\n\nExercise 17\n\nThis assignment was a reading and brief concept summary completed on Canvas. No deliverable is available for this assignment.\n\nExercise 18 & 19 (This link is under construction)\n\nIn this assignment we completed modeling tasks with the NY-Times COVID-19 and census data. Exercise 19 invovled tuning the basic model. This assignment was initially created and submitted as an R script and has been converted to a quarto document for the purposes of sharing documentation.\n\nExercise 20\n\nThis assignment was replaced by in-class attendance for extra credit. There is no deliverable for this assignment.\n\nExercise 21\n\nIn this assignment we practiced visualizing time series data using USGS data from the Cache la Poudre watershed.\n\nExercise 22\n\nIn this assignment we practiced forcast modeling with time series data using USGS data from the Cache la Poudre watershed.\n\nExercise 23\n\nIn this assignment we setup geospatial packages in R and learned about some common tools, data sources and packages using R for GIS. This assignment was initially created and submitted to Canvas and has been converted to a quarto document for the purposes of sharing documentation.\n\nExercise 24\n\nIn this assignment we practiced working with Lairmer County geospatial data."
  },
  {
    "objectID": "daily_exercises/exercise_23.html",
    "href": "daily_exercises/exercise_23.html",
    "title": "Exercise 23: Spatial Packages Prep",
    "section": "",
    "text": "library(sf)\n\nWarning: package 'sf' was built under R version 4.4.3\n\n\nLinking to GEOS 3.13.0, GDAL 3.10.1, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(terra)\n\nWarning: package 'terra' was built under R version 4.4.3\n\n\nterra 1.8.42\n\nsf::sf_extSoftVersion()\n\n          GEOS           GDAL         proj.4 GDAL_with_GEOS     USE_PROJ_H \n      \"3.13.0\"       \"3.10.1\"        \"9.5.1\"         \"true\"         \"true\" \n          PROJ \n       \"9.5.1\" \n\nterra::gdal()\n\n[1] \"3.10.1\"\n\n\n\nGDAL: Geospatial Data Abstraction Library\nGDAL is an open-source library/software for reading, writing, and transforming geospatial data formats. It drives anything and everything spatial and enables interoperability across platforms by standardizing access to many data formats.\n\n\nGEOS: Geometry Engine - Open Source\nGEOS is a C++ library for performing geometric operations on planar geometry objects. It was ported from the Java library Java Topology Suite (JTS). It handles topological operations like buffering, intersections, unions, and differences.\n\n\nPROJ: PROJ used to be PROJ.4\nPROJ is a library for performing coordinate transformations. It enables conversion between geographic and projection coordinate systems. It is responsible for the mathematics behind map projections and datum shifts, ensuring that data aligns with map resources correctly.\nDirect access to the command line (CLI) in a functional language like R enables reproducibility with version control, automation, compute efficiency, and the ability to pass R variables through CLI tools."
  },
  {
    "objectID": "daily_exercises/exercise_21.html",
    "href": "daily_exercises/exercise_21.html",
    "title": "Exercise 21: Vizualizing Time Series",
    "section": "",
    "text": "library(dataRetrieval)\n\nWarning: package 'dataRetrieval' was built under R version 4.4.3\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(zoo)\n\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(tsibble)\n\nWarning: package 'tsibble' was built under R version 4.4.3\n\n\nRegistered S3 method overwritten by 'tsibble':\n  method               from \n  as_tibble.grouped_df dplyr\n\nAttaching package: 'tsibble'\n\nThe following object is masked from 'package:zoo':\n\n    index\n\nThe following object is masked from 'package:lubridate':\n\n    interval\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, union\n\nlibrary(plotly)\n\nWarning: package 'plotly' was built under R version 4.4.3\n\n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(feasts)\n\nWarning: package 'feasts' was built under R version 4.4.3\n\n\nLoading required package: fabletools\n\n\nWarning: package 'fabletools' was built under R version 4.4.3\n\n# Bringing in Data\n# Timeseries Example\n# Cache la Poudre River at Mouth (USGS site 06752260)\npoudre_flow &lt;- readNWISdv(siteNumber = \"06752260\",    # Download data from USGS for site 06752260\n                          parameterCd = \"00060\",      # Parameter code 00060 = discharge in cfs)\n                          startDate = \"2013-01-01\",   # Set the start date\n                          endDate = \"2023-12-31\") |&gt;  # Set the end date\n  renameNWISColumns() |&gt;                              # Rename columns to standard names (e.g., \"Flow\", \"Date\")\n  mutate(Date = yearmonth(Date)) |&gt;                   # Convert daily Date values into a year-month format (e.g., \"2023 Jan\")\n  group_by(Date) |&gt;                                   # Group the data by the new monthly Date\n  summarise(Flow = mean(Flow))                       # Calculate the average daily flow for each month\n\nGET:https://waterservices.usgs.gov/nwis/dv/?site=06752260&format=waterml%2C1.1&ParameterCd=00060&StatCd=00003&startDT=2013-01-01&endDT=2023-12-31\n\n\n\n# 1. Convert to tsibble\npoudre_ts &lt;- poudre_flow %&gt;% \n  as_tsibble(index = Date)\n\n# 2. Plot and animate the time series\nts_plot &lt;- ggplot(poudre_ts, aes(x = Date, y = Flow)) +\n  geom_line(color = \"steelblue\") +\n  labs(title = \"Poudre River Flow\",\n       x = \"Date\",\n       y = \"Flow (cfs)\") +\n  theme_minimal()\n\nggplotly(ts_plot)\n\n\n\n\n# 3. Subseries Plot\nsubseries_plot &lt;- gg_subseries(poudre_ts, Flow) +\n  labs(\n    title = \"Montly Streamflow: Cache la Poudre River (2013-2023)\",\n    subtitle = \"Each panel show the avg. streamflow (cfs) for a given month over 10 years.\",\n    x = \"Date\",\n    y = \"Average Monthly Flow (cfs)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12),\n    strip.text = element_text(face = \"bold\"),\n    axis.text.x = element_text(angle = 90, hjust = 1, vjust = -1),\n  )\n\nggplotly(subseries_plot)\n\n\n\n\n\n\nThe subseries in this example represent months, and seasons are defined by the average monthly flow rate. In hydrology and river ecology seasons are really more binary than in human perception. There are wet and dry seasons as well as hot or cold seasons although the latter to a lesser extent.\n\n\n# 4. Decomposition using STL\ndecomp &lt;- poudre_ts %&gt;% \n  model(STL(Flow ~ season(window = \"periodic\"))) %&gt;% \n  components()\n\nautoplot(decomp)\n\n\n\n\n\n\n\n\n\nIn general the decomposition shows the flow decreasing over time but also becoming more volatile over time, particularly in the wet months. While the base flows do decrease slightly the trend shows a much clearer difference; such a difference may be better reflected in the consistency or stability of the flow reigeme."
  },
  {
    "objectID": "daily_exercises/exercise_15.html",
    "href": "daily_exercises/exercise_15.html",
    "title": "Daily Exercise 15: Data Splitting",
    "section": "",
    "text": "# 25 March 2025\n# This daily assignment uses the penguins dataset to practice seeding, splitting and cross validation.\n\n# Load necessary libraries and data\nlibrary(\"tidymodels\")\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.7     ✔ recipes      1.1.1\n✔ dials        1.3.0     ✔ rsample      1.3.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.1     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.2\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(\"rsample\")\nlibrary(\"palmerpenguins\")\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following object is masked from 'package:modeldata':\n\n    penguins\n\ndata(\"penguins\")\n\n# Clean and view the penguins df\npenguins &lt;- drop_na(penguins)\nstr(penguins)\n\ntibble [333 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:333] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6 ...\n $ bill_depth_mm    : num [1:333] 18.7 17.4 18 19.3 20.6 17.8 19.6 17.6 21.2 21.1 ...\n $ flipper_length_mm: int [1:333] 181 186 195 193 190 181 195 182 191 198 ...\n $ body_mass_g      : int [1:333] 3750 3800 3250 3450 3650 3625 4675 3200 3800 4400 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 1 2 1 2 2 ...\n $ year             : int [1:333] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\nglimpse(penguins)\n\nRows: 333\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n# Set the seed\nset.seed(101991) # Random seed borrowed from lecture\n\n#Extract the training and testing sets with 70/30 split stratified by species.\npenguins_strata &lt;- initial_split(penguins, strata = species, prop = 0.7)\n\n# Check test split\n100/333\n\n[1] 0.3003003\n\n# Extract split data\npenguins_train &lt;- training(penguins_strata)\npenguins_test &lt;- testing(penguins_strata)\n\n# Print tibbles for train/test data\npenguins_train\n\n# A tibble: 232 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           39.3          20.6               190        3650\n 5 Adelie  Torgersen           39.2          19.6               195        4675\n 6 Adelie  Torgersen           41.1          17.6               182        3200\n 7 Adelie  Torgersen           38.6          21.2               191        3800\n 8 Adelie  Torgersen           36.6          17.8               185        3700\n 9 Adelie  Torgersen           38.7          19                 195        3450\n10 Adelie  Torgersen           42.5          20.7               197        4500\n# ℹ 222 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\npenguins_test\n\n# A tibble: 101 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           36.7          19.3               193        3450\n 2 Adelie  Torgersen           38.9          17.8               181        3625\n 3 Adelie  Torgersen           34.6          21.1               198        4400\n 4 Adelie  Biscoe              37.8          18.3               174        3400\n 5 Adelie  Biscoe              37.7          18.7               180        3600\n 6 Adelie  Biscoe              38.8          17.2               180        3800\n 7 Adelie  Dream               37.2          18.1               178        3900\n 8 Adelie  Dream               39.2          21.1               196        4150\n 9 Adelie  Dream               40.8          18.4               195        3900\n10 Adelie  Biscoe              40.1          18.9               188        4300\n# ℹ 91 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# Resampling\nset.seed(3214) # Set resampling seed with random number from the lecture\n\n# Conduct 10-Fold Cross-Validation\nnrow(penguins_train) * 1/10\n\n[1] 23.2\n\nvfold_cv(penguins_train, v = 10)\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [208/24]&gt; Fold01\n 2 &lt;split [208/24]&gt; Fold02\n 3 &lt;split [209/23]&gt; Fold03\n 4 &lt;split [209/23]&gt; Fold04\n 5 &lt;split [209/23]&gt; Fold05\n 6 &lt;split [209/23]&gt; Fold06\n 7 &lt;split [209/23]&gt; Fold07\n 8 &lt;split [209/23]&gt; Fold08\n 9 &lt;split [209/23]&gt; Fold09\n10 &lt;split [209/23]&gt; Fold10\n\npenguin_folds &lt;- vfold_cv(penguins_train)\npenguin_folds$splits[1:3]\n\n[[1]]\n&lt;Analysis/Assess/Total&gt;\n&lt;208/24/232&gt;\n\n[[2]]\n&lt;Analysis/Assess/Total&gt;\n&lt;208/24/232&gt;\n\n[[3]]\n&lt;Analysis/Assess/Total&gt;\n&lt;209/23/232&gt;"
  },
  {
    "objectID": "daily_exercises/exercise_08.html",
    "href": "daily_exercises/exercise_08.html",
    "title": "Daily Exercise 08",
    "section": "",
    "text": "# 20 Feburary 2025\n# This is the R script for assignment 08 where the COVID-19 data from \n# assignment 06 will be visualized expanding on the visualization from\n# assignment 07.\n\n# Prepare packages\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(datasets)\nlibrary(scales)\n\n#Read in and store NY-Times Data\ncovid &lt;- readr::read_csv(\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\")\n\nRows: 2502832 Columns: 6\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): county, state, fips\ndbl  (2): cases, deaths\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Create state-region data frame\ndf &lt;- data.frame(region = state.region,\n                 abbr = state.abb,\n                 state = state.name)\n\n# Check data frame\nhead(df)\n\n  region abbr      state\n1  South   AL    Alabama\n2   West   AK     Alaska\n3   West   AZ    Arizona\n4  South   AR   Arkansas\n5   West   CA California\n6   West   CO   Colorado\n\n# Join df to covid data\ncovid_joined &lt;- inner_join(df,covid, by = \"state\")\n\n# Aggregate (Split-apply) COVID-19 data based on region.\ncovid_summary &lt;- covid_joined %&gt;% \n  group_by(region, date) %&gt;% #Split by region and date\n  summarize(\n    Cases = sum(cases/1000, na.rm = TRUE), #Apply sum function\n    Deaths = sum(deaths/1000, na.rm = TRUE)\n  )\n\n`summarise()` has grouped output by 'region'. You can override using the\n`.groups` argument.\n\n# Pivot data from to long format\ncovid_long &lt;- covid_summary %&gt;% \n  pivot_longer(cols = c(Cases, Deaths), \n              names_to = \"metric\", \n              values_to = \"count\")\n\n# Plot the long data\nus_regional_covid_trends = ggplot(covid_long, aes(x = date, y = count, color = region)) +\n  geom_line() +\n  facet_grid(metric~region, scales = \"free_y\",) +\n  labs(title = \"COVID-19 Cases and Deaths by Region\",\n       subtitle = \"Since 2020 (per 1,000 people)\",\n       caption = \"Based on NY-Times COVID-19 Data.\",\n       x = \"Date\",\n       y = \"Cumulative Cases\",\n       color = \"Regions\") +\n  theme_bw() + \n  theme(legend.position = \"none\",\n        axis.text.y = element_text(angle = 45, hjust = 1)) + \n  scale_x_date(date_breaks = \"8 month\", date_labels = \"%b %y\") +\n  scale_y_continuous(labels = label_number())\n  \n# Save plot as an image\n#ggsave(us_regional_covid_trends,\n     # file = \"img/us_regional_covid_trends.png\",\n     # width = 10,\n     # height = 6)"
  },
  {
    "objectID": "daily_exercises/exercise_07.html",
    "href": "daily_exercises/exercise_07.html",
    "title": "Daily Exercise 07",
    "section": "",
    "text": "# 20 Feburary 2025\n# ESS 330 - Daily Assignment 07\n# This file is a copy of the R script for assignment 07 where the COVID-19 data from \n# assignment 06 is visualized.\n\n## COVID-19 Data\n\n### Data\n\n# We are going to practice some data wrangling skills using a real-world \n# data set about COVID cases curated and maintained by the New York Times. The\n# data are archived on a GitHub repo [here](https://github.com/nytimes/covid-19-data). \n\n#Prepare libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Read-in and store NY-Times data\nurl &lt;- 'https://raw.githubusercontent.com/nytimes/covid-19-data/refs/heads/master/us-states.csv'\ncovid &lt;- read_csv(url)\n\nRows: 61942 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): state, fips\ndbl  (2): cases, deaths\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n### Question 1\n\n# Find the most recent date\nmax_date &lt;- max(covid$date)\nmost_recent_data &lt;- filter(covid, date == max_date)\n\n# Get the 6 states with the most cases\ntop_6_states &lt;- most_recent_data %&gt;%\n  arrange(desc(cases)) %&gt;% \n  slice(1:6) %&gt;% \n  pull(state)\n\n# Filter data to the top 6 states\ntop_6_states_data &lt;- covid %&gt;% \n  filter(state %in% top_6_states) %&gt;% \n  group_by(state, date) %&gt;% \n  summarize(state_cases = sum(cases, na.rm = TRUE))\n\n`summarise()` has grouped output by 'state'. You can override using the\n`.groups` argument.\n\n# Set up a gg plot\ntop_6_states_plot = ggplot(top_6_states_data, aes(x = date, y = state_cases, group = state, color = state)) +\n  geom_line() +\n  labs(\n    title = \"Cumulative Case Counts: COVID-19 Pandemic\",\n    caption = \"Based on NY-Times COVID-19 Data.\",\n    x = \"Date\",\n    y = \"Number of Cases\",\n    color = \"State\"\n  ) +\n  facet_wrap(~state) + \n  theme_bw() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1), \n    legend.position = \"none\"\n  ) + \n  scale_x_date(date_breaks = \"8 month\", date_labels = \"%b %y\")\n\n# Echo plot to dev env\ntop_6_states_plot\n\n\n\n\n\n\n\n# Save the top_6_states_plot as an image\n#ggsave(top_6_states_plot, \n      # file = \"images\\\\top_6_state_cases_plot.jpg\", \n      # width = 10,\n      # height = 6,\n      # units = c(\"in\"))\n\n### Question 2\n\n# Find daily total cases in the US\nus_daily_cases_data &lt;- covid %&gt;% \n  group_by(date) %&gt;% \n  summarize(us_cases = sum(cases, na.rm = TRUE))\n\n# Plot us_daily_cases_data\nus_daily_cases_plot = ggplot(us_daily_cases_data, aes(x = date, y = us_cases)) + \n  geom_col(color = \"darkred\") + \n  labs(\n    title = \"Cumulative Case Counts: US COVID-19 Pandemic\",\n    caption = \"Based on NY-Times COVID-19 Data.\",\n    author = \"Zachary Cramton\",\n    x = \"Date\",\n    y = \"Number of Cases\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_x_date(date_breaks = \"8 month\", date_labels = \"%b %y\")\n\n# Echo the plot to dev env\nus_daily_cases_plot\n\n\n\n\n\n\n\n# Save the us_daily_plot as an image\n#ggsave(us_daily_cases_plot,\n      # file = \"images\\\\us_daily_cases_plot.jpg\",\n      # width = 10,\n      # height = 10,\n      # units = c(\"in\"))"
  },
  {
    "objectID": "daily_exercises/exercise_11+12.html",
    "href": "daily_exercises/exercise_11+12.html",
    "title": "Daily Assignment 11/12: Air Quality and EDA Modeling",
    "section": "",
    "text": "library(tidyverse)  # For data wrangling\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(recipes)    # For data preprocessing\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\n\nAttaching package: 'recipes'\n\nThe following object is masked from 'package:stringr':\n\n    fixed\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(broom)      # For model diagnostics\nlibrary(ggpubr)     # For visualization\n\nWarning: package 'ggpubr' was built under R version 4.4.3\n\n# Load the airquality data set\ndata(\"airquality\")\n\n# Remove rows where Ozone is NA before preprocessing\nairquality &lt;- airquality %&gt;% drop_na(Ozone)\n\n\n# Part 1: Normality Testing\n# Explore data set structure\nstr(airquality)\n\n'data.frame':   116 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 28 23 19 8 7 16 ...\n $ Solar.R: int  190 118 149 313 NA 299 99 19 NA 256 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.9 8.6 13.8 20.1 6.9 9.7 ...\n $ Temp   : int  67 72 74 62 66 65 59 61 74 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 6 7 8 9 11 12 ...\n\nsummary(airquality)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 2.300   Min.   :57.00  \n 1st Qu.: 18.00   1st Qu.:113.5   1st Qu.: 7.400   1st Qu.:71.00  \n Median : 31.50   Median :207.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :184.8   Mean   : 9.862   Mean   :77.87  \n 3rd Qu.: 63.25   3rd Qu.:255.5   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n                  NA's   :5                                       \n     Month            Day       \n Min.   :5.000   Min.   : 1.00  \n 1st Qu.:6.000   1st Qu.: 8.00  \n Median :7.000   Median :16.00  \n Mean   :7.198   Mean   :15.53  \n 3rd Qu.:8.250   3rd Qu.:22.00  \n Max.   :9.000   Max.   :31.00  \n                                \n\n# Perform Shapiro-Wilk test for normality\nshapiro.test(airquality$Ozone)\n\n\n    Shapiro-Wilk normality test\n\ndata:  airquality$Ozone\nW = 0.87867, p-value = 2.79e-08\n\nshapiro.test(airquality$Temp)\n\n\n    Shapiro-Wilk normality test\n\ndata:  airquality$Temp\nW = 0.97944, p-value = 0.0719\n\nshapiro.test(airquality$Solar.R)\n\n\n    Shapiro-Wilk normality test\n\ndata:  airquality$Solar.R\nW = 0.93285, p-value = 2.957e-05\n\nshapiro.test(airquality$Wind)\n\n\n    Shapiro-Wilk normality test\n\ndata:  airquality$Wind\nW = 0.97898, p-value = 0.06537\n\n\nThe Shapiro-Wilk test is used to assess whether a data set follows a normal distribution. It is useful to confirm normality before conduting or running tests which assume normality (i.e. linear regressions).\nHypotheses for Shapiro-Wilk Tests: Null: The data is normally distributed. Alt.: The data is not normally distributed.\nIf the p-value is &gt; 0.05 then, FTR the null; the data is likely normal and doesn’t show significant non-normality. if the p-value is &lt; 0.05 the, reject the null; the data is likely not normally distributed as it shows significant differences from a normal distribution.\nIn this case, Ozone and Solar r. have p-values less than 0.05 meaning those data are likely not normally distributed. The Wind and Temp variables have a p-value &gt; 0.05 meaning the data are likely normally distributed.\n\n# Part 2: Data Transformation and Feature Engineering\n# Convert Month into seasons using case_when()\nairquality &lt;- airquality %&gt;%\n  mutate(Season = case_when(\n    Month %in% c(11, 12, 1) ~ \"Winter\",\n    Month %in% c(2, 3, 4) ~ \"Spring\",\n    Month %in% c(5, 6, 7) ~ \"Summer\",\n    Month %in% c(8, 9, 10) ~ \"Fall\"\n  ))\n\n# Check observations per season\ntable(airquality$Season)\n\n\n  Fall Summer \n    55     61 \n\n\nThere are 116 observations in the airquality data set, with 61 from the summer months and 55 from the fall months. There are no spring or winter records in the data.\n\n# Part 3: Data Preprocessing\n# Normalize predictor variables\nrec &lt;- recipe(Ozone ~ Temp + Solar.R + Wind + Season, data = airquality) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_impute_mean(all_numeric_predictors())\n\nprep_rec &lt;- prep(rec)\nprocessed_data &lt;- bake(prep_rec, new_data = NULL)\n\nNormalizing data rescales values to have a mean of 0 and standard deviation of 1. This allows predictors to contribute to a model equally instead of veriables with large magnitudes having more weight in the analysis.\nThe step_impute_mean() function from the recipies package is used to impute missing values with the mean of the variable.\nIt’s necessary to prep() and bake() the recipie because prep() calculates necessary statistics (i.e. mean for imputation and scaling factors for normalization) while bake applies the transformation. Without prep() theres no transformation to apply without bake() the transformation doesn’t get applied.\n\n# Part 4: Building a Linear Regression Model\nlm_model &lt;- lm(Ozone ~ ., data = processed_data)\nsummary(lm_model)  # Interpret coefficients, R-squared, and p-values\n\n\nCall:\nlm(formula = Ozone ~ ., data = processed_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.532 -14.806  -2.438  10.502  99.521 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    40.034      2.965  13.501  &lt; 2e-16 ***\nTemp           16.408      2.482   6.611 1.38e-09 ***\nSolar.R         5.038      2.184   2.307   0.0229 *  \nWind          -11.114      2.315  -4.801 4.96e-06 ***\nSeasonSummer    3.984      4.197   0.949   0.3445    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.33 on 111 degrees of freedom\nMultiple R-squared:  0.5965,    Adjusted R-squared:  0.5819 \nF-statistic: 41.02 on 4 and 111 DF,  p-value: &lt; 2.2e-16\n\n\nThis model is attempting to estimate changes in ozone based on the predictor variables. In this case, the coefficients represent the estimated change in ozone for a one-unit change in the associated predictor variable. A positive coefficient means that ozone will increase with increases in the predictor variable while a negative change indicates that ozone will decrease when the predictor variable increases. The R-squared number assesses how well the model explains variability in the y-axis/independent variable where 1 means the model explains all of the y variability and 0 indicated the model explains none of the y variability. Small p-values (&lt; 0.05) indicate that the predictor significantly impacts the modeled ozone levels while large p-values (&gt; 0.05) may indicate lack of statistical significance. The predicted ozone level when all predictors are zero is 40.034 (ppb)(intercept coeff.). The predictor variables from least to greatest impact are (coeff. magnitude): Temp, Wind and Solar R.\n\n# Part 5: Model Diagnostics\n# Supplement data with fitted values and residuals\nmodel_results &lt;- augment(lm_model, processed_data)\n\n# Extract residuals and visualize their distribution\nhistogram &lt;- ggplot(model_results, aes(.resid)) +\n  geom_histogram(bins = 20, fill = \"blue\", alpha = 0.5) +\n  ggtitle(\"Residuals Histogram\")\n\nresiduals_plot &lt;- ggqqplot(model_results$.resid) +\n  ggtitle(\"QQ Plot of Residuals\")\n\n# Arrange plots side by side\nggarrange(histogram, residuals_plot, ncol = 2, nrow = 1)\n\n\n\n\n\n\n\n# Scatter plot of actual vs. predicted values\nggscatter(model_results, x = \"Ozone\", y = \".fitted\",\n          add = \"reg.line\", conf.int = TRUE,\n          cor.coef = TRUE, cor.method = \"spearman\",\n          ellipse = TRUE)\n\n\n\n\n\n\n\n\nWith some large residuals, the predictions may not be particularly accurate. The median close to 0 suggests a relatively balanced residual distribution. Temp, Solar.R and Wind are statistically significant and contribute to the model while season is not and it may not be useful. Overall, the model has an R = 0.83 indicating a strong, positive correlation between the model and the observed data. With a very small p-value &lt;&lt; 0.05 the model is fairly strong and has a high likelihood of accurately predicting the observed data."
  },
  {
    "objectID": "daily_exercises/exercise_16.html",
    "href": "daily_exercises/exercise_16.html",
    "title": "Daily Exercise 16: Building a ML Workflow",
    "section": "",
    "text": "# Zachary Cramton\n# 25 March 2025\n# ESS 330 - Daily Assignment 15/16\n# This daily assignment uses the penguins dataset to practice seeding, splitting and cross validation.\n\n# Load necessary libraries and data\nlibrary(\"tidymodels\")\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.7     ✔ recipes      1.1.1\n✔ dials        1.3.0     ✔ rsample      1.3.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.1     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.2\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(\"rsample\")\nlibrary(\"palmerpenguins\")\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following object is masked from 'package:modeldata':\n\n    penguins\n\nlibrary(\"ranger\")\n\nWarning: package 'ranger' was built under R version 4.4.3\n\ndata(\"penguins\")\n\n# ---- Exercise 15: Data Cleaning and Splitting ---\n\n# Clean and view the penguins df\npenguins &lt;- drop_na(penguins)\nstr(penguins)\n\ntibble [333 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:333] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6 ...\n $ bill_depth_mm    : num [1:333] 18.7 17.4 18 19.3 20.6 17.8 19.6 17.6 21.2 21.1 ...\n $ flipper_length_mm: int [1:333] 181 186 195 193 190 181 195 182 191 198 ...\n $ body_mass_g      : int [1:333] 3750 3800 3250 3450 3650 3625 4675 3200 3800 4400 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 1 2 1 2 2 ...\n $ year             : int [1:333] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\nglimpse(penguins)\n\nRows: 333\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n# Set the seed\nset.seed(101991) # Random seed borrowed from lecture\n\n#Extract the training and testing sets with 70/30 split stratified by species.\npenguins_strata &lt;- initial_split(penguins, strata = species, prop = 0.7)\n\n# Check test split\n100/333\n\n[1] 0.3003003\n\n# Extract split data\npenguins_train &lt;- training(penguins_strata)\npenguins_test &lt;- testing(penguins_strata)\n\n# Print tibbles for train/test data\npenguins_train\n\n# A tibble: 232 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           39.3          20.6               190        3650\n 5 Adelie  Torgersen           39.2          19.6               195        4675\n 6 Adelie  Torgersen           41.1          17.6               182        3200\n 7 Adelie  Torgersen           38.6          21.2               191        3800\n 8 Adelie  Torgersen           36.6          17.8               185        3700\n 9 Adelie  Torgersen           38.7          19                 195        3450\n10 Adelie  Torgersen           42.5          20.7               197        4500\n# ℹ 222 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\npenguins_test\n\n# A tibble: 101 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           36.7          19.3               193        3450\n 2 Adelie  Torgersen           38.9          17.8               181        3625\n 3 Adelie  Torgersen           34.6          21.1               198        4400\n 4 Adelie  Biscoe              37.8          18.3               174        3400\n 5 Adelie  Biscoe              37.7          18.7               180        3600\n 6 Adelie  Biscoe              38.8          17.2               180        3800\n 7 Adelie  Dream               37.2          18.1               178        3900\n 8 Adelie  Dream               39.2          21.1               196        4150\n 9 Adelie  Dream               40.8          18.4               195        3900\n10 Adelie  Biscoe              40.1          18.9               188        4300\n# ℹ 91 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# Resampling\nset.seed(3214) # Set resampling seed with random number from the lecture\n\n# Conduct 10-Fold Cross-Validation\nnrow(penguins_train) * 1/10\n\n[1] 23.2\n\nvfold_cv(penguins_train, v = 10)\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [208/24]&gt; Fold01\n 2 &lt;split [208/24]&gt; Fold02\n 3 &lt;split [209/23]&gt; Fold03\n 4 &lt;split [209/23]&gt; Fold04\n 5 &lt;split [209/23]&gt; Fold05\n 6 &lt;split [209/23]&gt; Fold06\n 7 &lt;split [209/23]&gt; Fold07\n 8 &lt;split [209/23]&gt; Fold08\n 9 &lt;split [209/23]&gt; Fold09\n10 &lt;split [209/23]&gt; Fold10\n\npenguin_folds &lt;- vfold_cv(penguins_train)\npenguin_folds$splits[1:3]\n\n[[1]]\n&lt;Analysis/Assess/Total&gt;\n&lt;208/24/232&gt;\n\n[[2]]\n&lt;Analysis/Assess/Total&gt;\n&lt;208/24/232&gt;\n\n[[3]]\n&lt;Analysis/Assess/Total&gt;\n&lt;209/23/232&gt;\n\n# ---- Day 16: Model Fitting and Workflow ----\n\n# Define logistic regression model\nlog_reg_model &lt;- multinom_reg() %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"classification\")\n\n# Define random forest model\nrand_forest_model &lt;- rand_forest() %&gt;% \n  set_engine(\"ranger\") %&gt;%   # Default for rand_forest() model\n  set_mode(\"classification\")\n\n# Define recipe for pre-processing\npenguin_recipe &lt;- recipe(species ~., data = penguins_train) %&gt;% \n  step_normalize(all_numeric_predictors())   # Normalizes predictor vars for mean = 0 and variance = 1\n\n# Create workflows to keep models and recipes (preprocessing) together.\nlog_ref_wf &lt;- workflow() %&gt;% \n  add_model(log_reg_model) %&gt;%    # Adds log_reg_model to the wf\n  add_recipe(penguin_recipe)  # Adds normalization recipe\n\nrand_forest_wf &lt;- workflow() %&gt;% \n  add_model(rand_forest_model) %&gt;% # Adds rand_forest model to the wf\n  add_recipe(penguin_recipe)   # Adds normalization recipe\n\n# Compare models with a workflow set\nmodel_set &lt;- workflow_set(\n  preproc = list(recipe = penguin_recipe),   # Apply the same preprocessing (normalization) recipe to all models\n  models = list(logistic_regression = log_reg_model, \n                random_forest = rand_forest_model)\n)\n\n# Fit models using resampling\nset.seed(34215)   # Randomly selected seed\nresults &lt;- model_set %&gt;% \n  workflow_map(\"fit_resamples\", \n               resamples = penguin_folds, \n               metrics = metric_set(accuracy))   # Fit models using cross validation\n\n# Rank models by accuracy\nranked_results &lt;- rank_results(results, rank_metric = \"accuracy\")\nprint(ranked_results)   # Print results\n\n# A tibble: 2 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_logistic_… Prepro… accura… 0.987 0.00922    10 recipe       mult…     1\n2 recipe_random_fo… Prepro… accura… 0.983 0.00710    10 recipe       rand…     2\n\n# Comment: \n# Using a standard logistic_reg model the mean accuracy was ~0.638 with poor stability. \n  # The rand_forest model was far superior in this case.\n# As the logistic_reg model is set up for binary outcomes not multiclass classification,\n  # I used a different log_reg_model type.\n# The multinom_reg model allows for multiclass classificiation with the nnet engine \n  # handling more than two classes.\n\n# After switching to the multinom_reg model, it outperformed the random_forest \n  # model by ~0.004 in mean accuracy making it the better model for accuracy.\n# However, the random_forest model outperformed the multinom_reg model by ~0.002 \n  # in accuracy std. error making the random_forest model the better choice for stability."
  },
  {
    "objectID": "daily_exercises/exercise_22.html",
    "href": "daily_exercises/exercise_22.html",
    "title": "Exercise 22: Forcast Modeling with Time Series Data",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dataRetrieval)\n\nWarning: package 'dataRetrieval' was built under R version 4.4.3\n\nlibrary(lubridate)\nlibrary(timetk)\n\nWarning: package 'timetk' was built under R version 4.4.3\n\nlibrary(modeltime)\n\nWarning: package 'modeltime' was built under R version 4.4.3\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(prophet)\n\nWarning: package 'prophet' was built under R version 4.4.3\n\n\nLoading required package: Rcpp\n\nAttaching package: 'Rcpp'\n\nThe following object is masked from 'package:rsample':\n\n    populate\n\nLoading required package: rlang\n\n\nWarning: package 'rlang' was built under R version 4.4.3\n\n\n\nAttaching package: 'rlang'\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, flatten, flatten_chr, flatten_dbl, flatten_int, flatten_lgl,\n    flatten_raw, invoke, splice\n\n# Data import for: Cache la Poudre River at Mouth (USGS site 06752260)\n\n# Bringing in historical (training) data (2013-2023)\npoudre_hist_flow &lt;- readNWISdv(siteNumber = \"06752260\",    # Download data from USGS for site 06752260\n                          parameterCd = \"00060\",      # Parameter code 00060 = discharge in cfs)\n                          startDate = \"2013-01-01\",   # Set the start date\n                          endDate = \"2023-12-31\") %&gt;% # Set the end date\n  renameNWISColumns() %&gt;%                             # Rename columns to standard names (e.g., \"Flow\", \"Date\")\n  mutate(date = floor_date(Date, \"month\")) %&gt;%        # Convert to first day of each month\n  group_by(date) %&gt;%                                  # Group the data by the new monthly Date\n  summarize(Flow = mean(Flow, na.rm = TRUE)) %&gt;%      # Calculate the monthly average flow\n  ungroup()\n\nGET:https://waterservices.usgs.gov/nwis/dv/?site=06752260&format=waterml%2C1.1&ParameterCd=00060&StatCd=00003&startDT=2013-01-01&endDT=2023-12-31\n\n# Bring in recent (testing) data (2024)\npoudre_2024_obs_flow &lt;- readNWISdv(siteNumber = \"06752260\",\n                          parameterCd = \"00060\",      \n                          startDate = \"2024-01-01\",  \n                          endDate = \"2024-12-31\") %&gt;% \n  renameNWISColumns() %&gt;%                              \n  mutate(date = floor_date(Date, \"month\")) %&gt;%        \n  group_by(date) %&gt;%                                 \n  summarize(Flow = mean(Flow, na.rm = TRUE)) %&gt;%     \n  ungroup()\n\nGET:https://waterservices.usgs.gov/nwis/dv/?site=06752260&format=waterml%2C1.1&ParameterCd=00060&StatCd=00003&startDT=2024-01-01&endDT=2024-12-31\n\n\n\n# Log normalization for training data\npoudre_hist_flow &lt;- poudre_hist_flow %&gt;% \n  mutate(log_flow = log1p(Flow))\n\n# Define forcasting models\n# Prophet model\nprophet_fit &lt;- prophet_reg() %&gt;% \n  set_engine(\"prophet\") %&gt;% \n  fit(log_flow ~ date, data = poudre_hist_flow)\n\nDisabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this.\n\n\nDisabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.\n\n# ARIMA model\narima_fit &lt;- arima_reg() %&gt;% \n  set_engine(\"auto_arima\") %&gt;% \n  fit(log_flow ~ date, data = poudre_hist_flow)\n\nfrequency = 12 observations per 1 year\n\n# Combine models into modeltime table\nmodels_tbl &lt;- modeltime_table(prophet_fit, arima_fit)\n\n# Fortcast the next 12 months w/ test data\nfuture_dates &lt;- poudre_hist_flow %&gt;% \n  future_frame(.date_var = date, .length_out = 12)\n\nforcast_tbl &lt;- models_tbl %&gt;% \n  modeltime_forecast(\n    new_data = future_dates,\n    actual_data = poudre_hist_flow,\n    keep_data = TRUE\n  ) %&gt;% \n  mutate(.value = expm1(.value)) # Back-transform predictions to original scale\n  \n# Join predicted vs observed for 2024\ncomparison_tbl &lt;- forcast_tbl %&gt;% \n  filter(.key == \"prediction\") %&gt;% \n  select(.model_desc, .index, .value) %&gt;% \n  rename(\n    predicted = .value,\n    date = .index,\n    model_desc = .model_desc\n  ) %&gt;% \n  inner_join(poudre_2024_obs_flow, by = \"date\") %&gt;% \n  rename(observed = Flow) %&gt;% \n    mutate(model_desc = case_when(\n    str_detect(model_desc, \"ARIMA\") ~ \"ARIMA\",\n    str_detect(model_desc, \"PROPHET\") ~ \"Prophet\",\n    TRUE ~ model_desc))\n\n# Compute R-sq values\ncomparison_tbl %&gt;%  \n  group_by(model_desc) %&gt;% \n  summarize(r2 = summary(lm(observed ~ predicted))$r.squared)\n\n# A tibble: 2 × 2\n  model_desc    r2\n  &lt;chr&gt;      &lt;dbl&gt;\n1 ARIMA      0.922\n2 Prophet    0.880\n\n# Plot Predicted vs Observed Forcast Values\nggplot(comparison_tbl, aes(x = predicted, y = observed, color = model_desc)) +\n  geom_point(size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, linetype = \"solid\") +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\") +\n  labs(\n    title = \"Forcast (Predicted) vs Observed Monthly Flow for 2024\",\n    subtitle = \"Model comparison using ARIMA and Prophet\",\n    x = \"Predicted Flow (cfs)\",\n    y = \"Observed Flow (cfs)\",\n    color = \"Model\"\n  ) +\n  theme_minimal() +\n  theme(legend.position =  \"top\",\n        plot.title = element_text(hjust = 0.5),  # Center title\n    plot.subtitle = element_text(hjust = 0.5)  # Center\n    ) +\n  coord_fixed(ratio = 1) +   # Ensures 1:1 aspect ratio\n  coord_cartesian(xlim= c(0, 600), ylim = c(0,600)) # Fix axis scales\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "daily_exercises/exercise_24.html",
    "href": "daily_exercises/exercise_24.html",
    "title": "Exercise 24: Larimer County Cities",
    "section": "",
    "text": "WIP Expected Completion by 2020/04/28"
  },
  {
    "objectID": "labs/lab_03.html",
    "href": "labs/lab_03.html",
    "title": "Lab 3: COVID-19",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(flextable)\n\n\nAttaching package: 'flextable'\n\nThe following object is masked from 'package:purrr':\n\n    compose\n\nlibrary(zoo)\n\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(maps)\n\nWarning: package 'maps' was built under R version 4.4.3\n\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(patchwork)   # For Q8\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\nlibrary(sf)   # For Q8 visualization\n\nWarning: package 'sf' was built under R version 4.4.3\n\n\nLinking to GEOS 3.13.0, GDAL 3.10.1, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(viridis)   # For easier time-based spatial mapping coloring\n\nWarning: package 'viridis' was built under R version 4.4.3\n\n\nLoading required package: viridisLite\n\nAttaching package: 'viridis'\n\nThe following object is masked from 'package:maps':\n\n    unemp"
  },
  {
    "objectID": "labs/lab_03.html#question-1-public-data",
    "href": "labs/lab_03.html#question-1-public-data",
    "title": "Lab 3: COVID-19",
    "section": "Question 1: Public Data",
    "text": "Question 1: Public Data\n\n# Read in and store NY Times US county covid data\ncovid_url &lt;- \"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\"\nus_covid_data &lt;- read_csv(covid_url)\n\nRows: 2502832 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): county, state, fips\ndbl  (2): cases, deaths\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Read in and store US Census data\ncensus_url &lt;- \"https://www2.census.gov/programs-surveys/popest/datasets/2020-2023/counties/totals/co-est2023-alldata.csv\"\nus_census &lt;- read_csv(census_url)\n\nRows: 3195 Columns: 67\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): SUMLEV, STATE, COUNTY, STNAME, CTYNAME\ndbl (62): REGION, DIVISION, ESTIMATESBASE2020, POPESTIMATE2020, POPESTIMATE2...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Check data structure to ensure it's loaded properly (remove the \"#\" on the line below to see output)\n  #str(covid_data)\n  #str(us_census)\n\nEasily accessible, open source data provides the public with a means to hold the government, regulatory agencies and industry accountable. Historic data is particularly important for informing decisions in the present to avoid repeating historical mistakes. The present purge of information from numerous governmental websites is an excellent example of data loss benefiting a certain group at the detriment of those who wish to use previously collected data and established facts to contest ongoing changes. If you can’t point to data and show it exists than you don’t have a claim. If you don’t have a claim, you don’t have a cause. If you don’t have a cause, you have nothing to fight for and become aimless. Causes can exist on hope for a time but at some point the lact of data and mutually understood facts will lead to chaos and disorder."
  },
  {
    "objectID": "labs/lab_03.html#question-2-daily-summary",
    "href": "labs/lab_03.html#question-2-daily-summary",
    "title": "Lab 3: COVID-19",
    "section": "Question 2: Daily Summary",
    "text": "Question 2: Daily Summary\n\n# Define state and date for the analysis\nstate_name &lt;- \"Colorado\"\ntarget_date_1 &lt;- \"2022-02-01\"\n\n# Convert date column to Date type\nus_covid_data$date &lt;- as.Date(us_covid_data$date)\n\n# Create a subset limiting data to Colorado with new case and death information.\nco_covid_data &lt;- us_covid_data %&gt;% \n  filter(state == state_name) %&gt;%              # Filter Colorado data\n  arrange(county, date) %&gt;%                       # Collate data for each county and date\n  mutate(new_cases = cases - lag(cases),       # Create new column for new cases\n         new_deaths = deaths - lag(deaths))    # Create new column for new deaths\n\n# Create tables displaying the 5 counties with the highest cum/new cases.\n\n# Counties with the worst cases\nworst_cum_cases &lt;- co_covid_data %&gt;%\n  filter(date == target_date_1) %&gt;% \n  slice_max(cases, n = 5) %&gt;% \n  select(-state, -fips)\n\n#Counties with the worst deaths\nworst_new_cases &lt;- co_covid_data %&gt;%\n  filter(date == target_date_1) %&gt;% \n  slice_max(new_cases, n = 5) %&gt;% \n  select(-state, -fips)\n\n# Print worst cumulative cases table\nflextable(worst_cum_cases) %&gt;%\n  set_header_labels(\n    date = \"Date\",\n    county = \"County\",\n    cases = \"Cumulative Cases\",\n    deaths = \"Cumulative Deaths\",\n    new_cases = \"New Cases\",\n    new_deaths = \"New Deaths\"\n  ) %&gt;% \n  set_caption(\"Top 5 Colorado Counties for Cumulative Cases\") %&gt;% \n  align(part = \"all\", align = \"center\")\n\nDateCountyCumulative CasesCumulative DeathsNew CasesNew Deaths2022-02-01El Paso170,6731,51863072022-02-01Denver159,0221,19438992022-02-01Arapahoe144,2551,17240102022-02-01Adams126,7681,22432622022-02-01Jefferson113,2401,2192917\n\n#Print worst new cases table\nflextable(worst_new_cases) %&gt;% \n    set_header_labels(\n    date = \"Date\",\n    county = \"County\",\n    cases = \"Cumulative Cases\",\n    deaths = \"Cumulative Deaths\",\n    new_cases = \"New Cases\",\n    new_deaths = \"New Deaths\"\n  ) %&gt;% \n  set_caption(\"Top 5 Colorado Counties for New Cases\") %&gt;% \n  align(part = \"all\", align = \"center\")\n\nDateCountyCumulative CasesCumulative DeathsNew CasesNew Deaths2022-02-01El Paso170,6731,51863072022-02-01Arapahoe144,2551,17240102022-02-01Denver159,0221,19438992022-02-01Adams126,7681,22432622022-02-01Jefferson113,2401,2192917"
  },
  {
    "objectID": "labs/lab_03.html#question-3-normalizing-data",
    "href": "labs/lab_03.html#question-3-normalizing-data",
    "title": "Lab 3: COVID-19",
    "section": "Question 3: Normalizing Data",
    "text": "Question 3: Normalizing Data\n\n#Refine US Census Data\nus_census_formatted &lt;- us_census %&gt;% \n  # Remove state level data\n  filter(COUNTY != \"000\") %&gt;%    \n  # Reformat FIPS data from 2-3 to 5 digit combined strings.\n  mutate(\n    STATE = sprintf(\"%02d\", as.numeric(STATE)),     # Format state to 2 digits\n    COUNTY = sprintf(\"%03d\", as.numeric(COUNTY)),   # Format county to 3 digits\n    fips = paste0(STATE, COUNTY)   # Combine state and county codes to get full 5 digit FIPS code.\n  )\n\n# Select 2021 data\nus_census_2021 &lt;- us_census_formatted %&gt;% \n  \n  # Keep only columns with \"NAME\" or \"2021\" (or the FIP Column)\n  select(\n         contains(\"NAME\"),\n         contains(\"2021\"),\n         fips)   \n\n\n# Explore Census and CO COVID Data\nstr(us_census_2021)\n\ntibble [3,144 × 19] (S3: tbl_df/tbl/data.frame)\n $ STNAME               : chr [1:3144] \"Alabama\" \"Alabama\" \"Alabama\" \"Alabama\" ...\n $ CTYNAME              : chr [1:3144] \"Autauga County\" \"Baldwin County\" \"Barbour County\" \"Bibb County\" ...\n $ POPESTIMATE2021      : num [1:3144] 59203 239439 24533 22359 59079 ...\n $ NPOPCHG2021          : num [1:3144] 288 6212 -436 171 -28 ...\n $ BIRTHS2021           : num [1:3144] 686 2337 270 240 654 ...\n $ DEATHS2021           : num [1:3144] 696 2948 390 325 875 ...\n $ NATURALCHG2021       : num [1:3144] -10 -611 -120 -85 -221 -49 -70 -593 -200 -188 ...\n $ INTERNATIONALMIG2021 : num [1:3144] 15 105 0 1 9 1 5 12 22 7 ...\n $ DOMESTICMIG2021      : num [1:3144] 242 6972 -313 254 141 ...\n $ NETMIG2021           : num [1:3144] 257 7077 -313 255 150 ...\n $ RESIDUAL2021         : num [1:3144] 41 -254 -3 1 43 4 5 86 21 2 ...\n $ GQESTIMATES2021      : num [1:3144] 484 3351 2248 1994 616 ...\n $ RBIRTH2021           : num [1:3144] 11.62 9.89 10.91 10.78 11.07 ...\n $ RDEATH2021           : num [1:3144] 11.8 12.5 15.8 14.6 14.8 ...\n $ RNATURALCHG2021      : num [1:3144] -0.169 -2.585 -4.848 -3.816 -3.74 ...\n $ RINTERNATIONALMIG2021: num [1:3144] 0.254 0.4443 0 0.0449 0.1523 ...\n $ RDOMESTICMIG2021     : num [1:3144] 4.1 29.5 -12.65 11.4 2.39 ...\n $ RNETMIG2021          : num [1:3144] 4.35 29.95 -12.65 11.45 2.54 ...\n $ fips                 : chr [1:3144] \"01001\" \"01003\" \"01005\" \"01007\" ...\n\nstr(co_covid_data)\n\ntibble [49,527 × 8] (S3: tbl_df/tbl/data.frame)\n $ date      : Date[1:49527], format: \"2020-03-12\" \"2020-03-13\" ...\n $ county    : chr [1:49527] \"Adams\" \"Adams\" \"Adams\" \"Adams\" ...\n $ state     : chr [1:49527] \"Colorado\" \"Colorado\" \"Colorado\" \"Colorado\" ...\n $ fips      : chr [1:49527] \"08001\" \"08001\" \"08001\" \"08001\" ...\n $ cases     : num [1:49527] 2 3 6 6 8 10 10 10 12 14 ...\n $ deaths    : num [1:49527] 0 0 0 0 0 0 0 0 0 0 ...\n $ new_cases : num [1:49527] NA 1 3 0 2 2 0 0 2 2 ...\n $ new_deaths: num [1:49527] NA 0 0 0 0 0 0 0 0 0 ...\n\n\nThe two data frames being explored are US Census and US COVID data. The Census data has not been processed much while the COVID data has been reduced to contain only the data essential to the lab. The COVID data is narrower but much longer in its current form whereas the census data is much wider but shorter. The census data has multiple years of typical census record data whiel the covid data spans certain dates during the peak of the pandemic. The “fips” header is shared with the same 5-digit fips identifier.\n\n# Find range of CO pops in 2021\n# Filter for CO FIPS code (08)\nco_pop &lt;- us_census %&gt;% \n  filter(STATE == \"08\", COUNTY != \"000\") %&gt;% \n  group_by(COUNTY)\n\n# Calculate population range in CO in 2021\nco_pop_range &lt;- range(co_pop$`POPESTIMATE2021`, na.rm = TRUE)\n\n#Print colorado pop range\ncat(\"Range of populations in Colorado counties in 2021:\", co_pop_range)\n\nRange of populations in Colorado counties in 2021: 741 737287\n\n\nIn 2021 Colorado’s least populous county had 741 permanent residents and the most populous county had 737,287 permanent residents.\n\n# Join US Census data with CO covid data for 2021\nco_combined &lt;- co_covid_data %&gt;% \n  left_join(us_census_2021, by = \"fips\") %&gt;% \n  # Rename US Census Headers\n  rename(\n         pop_2021 = POPESTIMATE2021,\n         births_2021 = BIRTHS2021,\n         deaths_2021 = DEATHS2021\n         ) %&gt;% \n  # Calculate per capita (pc) statistics\n  mutate(\n         pc_cum_cases = cases / pop_2021,   # cumulative cases per capita\n         pc_cum_deaths = deaths / pop_2021,  # cumulative deaths per capita\n         pc_new_cases = new_cases / pop_2021,   # new cases per capita\n         pc_new_deaths = new_deaths / pop_2021,  # new deaths per capita\n        ) %&gt;% \n  select(1:8, 11, 13:14, 27:30)\n  \n# Narrow df to further remove superfluous columns from census data\nco_combined_abbr &lt;- co_combined %&gt;% \n  select(-births_2021, -deaths_2021)\n\n# Generate tables (2) for the 5 counties with highest cumulative and new cases par capita for a target date.\n# Set new target date (if desired)\ntarget_date_2 &lt;- \"2021-01-01\"\n\n# Counties with the worst cumulative cases\nworst_pc_cum_cases &lt;- co_combined_abbr %&gt;%\n  filter(date == target_date_2) %&gt;% \n  slice_max(pc_cum_cases, n = 5) %&gt;% \n  select(-state, -fips)\n\n#Counties with the worst new cases\nworst_pc_new_cases &lt;- co_combined_abbr %&gt;%\n  filter(date == target_date_2) %&gt;% \n  slice_max(pc_new_cases, n = 5) %&gt;% \n  select(-state, -fips)\n\n# Print worst cumulative cases table\nflextable(worst_pc_cum_cases) %&gt;%\n  set_header_labels(\n    date = \"Date\",\n    county = \"County\",\n    cases = \"Cumulative Cases\",\n    deaths = \"Cumulative Deaths\",\n    new_cases = \"New Cases\",\n    new_deaths = \"New Deaths\",\n    pc_cum_cases = \"Cumulative Cases Per Capita\",\n    pc_cum_deaths = \"Cumulative Deaths Per Capita\",\n    pc_new_cases = \"New Cases Per Capita\",\n    pc_new_deaths = \"New Deaths Per Capita\"\n  ) %&gt;%\n  set_caption(\"Top 5 Colorado Counties for Cumulative Cases Per Capita\") %&gt;% \n  align(part = \"all\", align = \"center\")\n\nDateCountyCumulative CasesCumulative DeathsNew CasesNew Deathspop_2021Cumulative Cases Per CapitaCumulative Deaths Per CapitaNew Cases Per CapitaNew Deaths Per Capita2021-01-01Crowley1,660121305,7350.289450740.00209241500.00226678290.000000000002021-01-01Bent1,126128905,3390.210900920.00224761190.01666978830.000000000002021-01-01Logan3,2495511121,0060.154670090.00261829950.00052365990.000047605452021-01-01Lincoln7833-1205,4730.143065960.0005481454-0.00219258180.000000000002021-01-01Fremont4,6812019149,2370.095070780.00040619860.00038588870.00002030993\n\n#Print worst new cases table\nflextable(worst_pc_new_cases) %&gt;% \n  set_header_labels(\n    date = \"Date\",\n    county = \"County\",\n    pop_2021 = \"Population (2021)\",\n    cases = \"Cumulative Cases\",\n    deaths = \"Cumulative Deaths\",\n    new_cases = \"New Cases\",\n    new_deaths = \"New Deaths\",\n    pc_cum_cases = \"Cumulative Cases Per Capita\",\n    pc_cum_deaths = \"Cumulative Deaths Per Capita\",\n    pc_new_cases = \"New Cases Per Capita\",\n    pc_new_deaths = \"New Deaths Per Capita\"\n  ) %&gt;% \n  set_caption(\"Top 5 Colorado Counties for New Cases Per Capita\") %&gt;% \n  align(part = \"all\", align = \"center\")\n\nDateCountyCumulative CasesCumulative DeathsNew CasesNew DeathsPopulation (2021)Cumulative Cases Per CapitaCumulative Deaths Per CapitaNew Cases Per CapitaNew Deaths Per Capita2021-01-01Bent1,126128905,3390.210900920.00224761190.01666978802021-01-01Sedgwick1342802,3260.057609630.00085984520.00343938102021-01-01Chaffee1,0722152019,7410.054303230.00106377590.00263411202021-01-01Crowley1,660121305,7350.289450740.00209241500.00226678302021-01-01Mineral441209290.047362760.00107642630.0021528530"
  },
  {
    "objectID": "labs/lab_03.html#quesiton-4-rolling-thresholds",
    "href": "labs/lab_03.html#quesiton-4-rolling-thresholds",
    "title": "Lab 3: COVID-19",
    "section": "Quesiton 4: Rolling Thresholds",
    "text": "Quesiton 4: Rolling Thresholds\n\n# Get most recent 14 day data\nlatest_date &lt;- max(co_combined_abbr$date, na.rm = TRUE)   # Get the latest date in the data set\ntwo_week_data &lt;- co_combined_abbr %&gt;% \n  filter(date &gt;= (latest_date - 13))   # Filter for the last 14 days\n\n# Summarize new cases per 100,000 residents\ntwo_wk_summary &lt;- two_week_data %&gt;% \n  group_by(county) %&gt;% \n  summarize(total_cases_14d = sum(new_cases, na.rm = TRUE),\n            population = first(pop_2021)) %&gt;% \n  mutate(cases_per_100k = (total_cases_14d / population) * 100000)\n\n# Find the 5 worst Colorado counties for 14-day new case numbers\ntop_5_worst_14_day_counties &lt;- two_wk_summary %&gt;% \n  slice_max(order_by = cases_per_100k, n = 5)\n\n# Print 5 worst counties for 14-day total new cases\nflextable(top_5_worst_14_day_counties) %&gt;% \n  set_header_labels(\n    county = \"County\",\n    total_cases_14d = \"New Cases (14 Days)\",\n    population = \"Population\",\n    cases_per_100k = \"Cases Per 100,000\"\n  ) %&gt;% \n  set_caption(\"Top 5 Colorado Counties for Total New Cases in the Past 14 Days\") %&gt;% \n  align(part = \"all\", align = \"center\") %&gt;% \n  autofit()\n\nCountyNew Cases (14 Days)PopulationCases Per 100,000Mineral8929861.1410Boulder2,437327,084745.0685Larimer1,943362,747535.6350Denver3,784711,467531.8588Jefferson2,646580,703455.6546\n\n# Count counties meeting the watch list condition (&gt;100 cases per 100,000)\nwatchlist_count &lt;- two_wk_summary %&gt;% \n  filter(cases_per_100k &gt; 100) %&gt;%\n  nrow()\n\n# Print the number of counties not meeting the watchlist condition.\ncat(\"Number of counties meeting the watchlist condition:\", watchlist_count)\n\nNumber of counties meeting the watchlist condition: 53"
  },
  {
    "objectID": "labs/lab_03.html#question-5-death-toll",
    "href": "labs/lab_03.html#question-5-death-toll",
    "title": "Lab 3: COVID-19",
    "section": "Question 5: Death toll",
    "text": "Question 5: Death toll\n\n# Find the percentage of total deaths in 2021 that were from COVID by Colorado county\nco_combined_exp &lt;- co_combined %&gt;% \n  filter(year(date) == (2021)) %&gt;%                                             # Limit analysis to 2021 data\n\n  # Summarize total deaths and total covid deaths per county\n  group_by(county) %&gt;%                                                 # Sort by county\n  summarize(total_covid_deaths_2021 = sum(deaths, na.rm = TRUE),       # Total covid deaths per county\n            total_deaths_2021 = sum(deaths_2021, na.rm = TRUE)) %&gt;%    # Total deaths per county\n  \n  # Create new column for percentage of deaths attributed to COVID\n  mutate(covid_death_percentage = (total_covid_deaths_2021 / total_deaths_2021) * 100)\n  \n# Identify high impact counties where &gt;20% of deaths were caused by COVID\nhigh_impact_counties &lt;- co_combined_exp %&gt;% \n  filter(covid_death_percentage &gt;= 20)\n\n# Plot high impact counties\nggplot(high_impact_counties, aes(x = reorder(county, covid_death_percentage), y = covid_death_percentage)) +\n  geom_col(fill = \"darkred\") +\n  labs(title = \"Colorado Counties Where COVID-19 Caused &gt;20% of Total Deaths in 2021\",\n       x = \"County\",\n       y = \"Percentage of Total Deaths Attributed to COVID\") +\n  theme_minimal()"
  },
  {
    "objectID": "labs/lab_03.html#question-6-multi-state",
    "href": "labs/lab_03.html#question-6-multi-state",
    "title": "Lab 3: COVID-19",
    "section": "Question 6: Multi-State",
    "text": "Question 6: Multi-State\n\n# Examine data from multiple states\nfour_state_covid_data &lt;- us_covid_data %&gt;% \n  group_by(date, state) %&gt;%            # Groups by day and state\n  summarize(cases = sum(cases), .groups = \"drop\") %&gt;%    # Sums the total cases per state per day\n  filter(state %in% c(\"New York\", \"Ohio\", \"Colorado\", \"Alabama\")) %&gt;%   # Selected the target states\n  group_by(state) %&gt;%   # Ensures state data is separated.\n  mutate(new_cases = cases - lag(cases),   # Computes the new cases per day (done previously in this lab but this section uses the original data).\n         roll_mean = rollmean(new_cases, k = 7, align = \"right\", fill = NA)) %&gt;%  # Calculates a 7-day rolling avg. of new cases to smooth daily fluctuations and account for delayed testing data.\n  ungroup()\n\n# Plot the COVID data for the four selected states separately for each state with bar charts and line graphs.\nggplot(four_state_covid_data, aes(x = date)) +   \n  geom_col(aes(y = new_cases), fill = \"darkred\", col = NA, na.rm = TRUE) +\n  geom_line(aes(y = roll_mean), col = \"black\", linewidth = 1, na.rm = TRUE)  +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_x_date(date_breaks = \"6 month\", date_labels = \"%b %y\") +\n  facet_wrap(~state, nrow = 2, scales = \"free_y\") +\n  labs(title = \"Daily COVID-19 Cases\", x = \"Date\", y = \"Case Count\")\n\n\n\n\n\n\n\n# Compare cases per capita in each state\n\n# Bring in US Census data\nfour_state_census_data &lt;- us_census %&gt;%\n  filter(STNAME %in% c(\"New York\", \"Ohio\", \"Colorado\", \"Alabama\"),   # Selected the target states\n         COUNTY != \"000\") %&gt;%   # Remove state level census data\n  \n  # Sort and sum for the 2021 population estimate in the selected states\n  group_by(STNAME) %&gt;%   # Ensures state data is separated.\n  summarize(state_pop = sum(POPESTIMATE2021)) # Summarize state population\n\n# Merge COVID and Census (Pop) data frames  \nfour_state_combined_data &lt;- four_state_covid_data %&gt;% \n  inner_join(four_state_census_data, by = c(\"state\" = \"STNAME\")) %&gt;% # Merge data frames with the shared fips data\n  mutate(state_pc_new_cases = new_cases / state_pop) %&gt;%  # Create new column in the df for state per capita new cases\n  \n  # Calc 7-day rolling mean for new cases per capita\n  arrange(state, date) %&gt;% \n  group_by(state) %&gt;% \n  mutate(pc_roll_mean = rollmean(state_pc_new_cases, k = 7, align = \"right\", fill = NA)) %&gt;% \n  ungroup()\n  \n# Plot the per capita COVID data for the four selected states separately for each state with bar charts and line graphs.\nggplot(four_state_combined_data, aes(x = date)) +   \n  geom_col(aes(y = state_pc_new_cases), fill = \"darkred\", col = NA, na.rm = TRUE) +\n  geom_line(aes(y = pc_roll_mean), col = \"black\", linewidth = 1, na.rm = TRUE)  +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_x_date(date_breaks = \"6 month\", date_labels = \"%b %y\") +\n  facet_wrap(~state, nrow = 2, scales = \"free_y\") +\n  labs(title = \"Daily COVID-19 Cases Per Capita\", x = \"Date\", y = \"Case Count Per Capita\")\n\n\n\n\n\n\n\n\nScaling by population tempered the apparent severity of new COVID-19 cases in states with high populations where there were more people to get sick. It made Alabama, with a relatively lower population look much worse because per person more people were getting sick by ~150%."
  },
  {
    "objectID": "labs/lab_03.html#quesiton-7-space-time",
    "href": "labs/lab_03.html#quesiton-7-space-time",
    "title": "Lab 3: COVID-19",
    "section": "Quesiton 7: Space & Time",
    "text": "Quesiton 7: Space & Time\n\n# Read in COVID-19 spatial data\nspatial_covid_url &lt;- \"https://raw.githubusercontent.com/mikejohnson51/csu-ess-330/refs/heads/main/resources/county-centroids.csv\"\nus_centroid_data &lt;- read_csv(spatial_covid_url, show_col_types = FALSE)\n\n# Join spatial data with the NYT US COVID-19 data\nus_centroid_covid_data &lt;- us_covid_data %&gt;% \n  inner_join(us_centroid_data, by = \"fips\")  # Join us covid data with spatial data using fips\n\n# Calculate the weighted mean center (\"epicenter\") of the data for each date using lat and long.\nweighted_center_data &lt;- us_centroid_covid_data %&gt;% \n  group_by(date) %&gt;% \n  reframe(\n    total_cases = sum(cases, na.rm = TRUE),   # Total cases for each date\n    wc_lat = sum(LAT * cases, na.rm = TRUE) / total_cases,   # Weighted mean y coord\n    wc_lon = sum(LON * cases, na.rm = TRUE) / total_cases,   # Weighted mean x coord\n    month = format(date, \"%m\")   # Extract month from the date\n  ) %&gt;% \n  group_by(date) %&gt;% \n  arrange(date) %&gt;%    # Sort df in chronological order\n  mutate(d = 1:n())   # Assign sequential numbers to the dates.\n  \n# Plot the weighted mean centers\nggplot(weighted_center_data) +\n  borders(\"state\", fill = \"gray90\", colour = \"white\") +   # Background US State map\n  geom_point(data = weighted_center_data, aes(x = wc_lon, y = wc_lat, size = total_cases), color = \"red\", alpha = 0.25) +    # Epicenters\n  scale_color_viridis_d() + #   Discrete color scale for time\n  theme_minimal() +\n  theme(legend.position = \"top\") +\n  labs(color = \"Month\", size = \"Cases\", x = \"\", y = \"\", title = \"Weighted Mean Center of COVID-19 Cases Over Time\")\n\n\n\n\n\n\n\n\nThe weighted mean epicenter of the COVID-19 pandemic in the US was concentrated on the southern Midwest. Over time it centered on Missouri and Arkansas. This makes sense given the generally lower quarantine regulations found in Southern states and the high case counts found in the generally higher density Eastern States averaged against the lower density but high population Western States, California in particular."
  },
  {
    "objectID": "labs/lab_03.html#question-8-cases-vs.-deaths",
    "href": "labs/lab_03.html#question-8-cases-vs.-deaths",
    "title": "Lab 3: COVID-19",
    "section": "Question 8: Cases vs. Deaths",
    "text": "Question 8: Cases vs. Deaths\n\n# Merge centroid data with COVID-19 Data for Colorado by fips code\nco_centroid_covid_data &lt;- us_covid_data %&gt;% \n  filter(state == \"Colorado\") %&gt;% \n  inner_join(us_centroid_data, by = \"fips\")\n\n# Set target date 3 from question 3 (target date 1) or input custom value.\ntarget_date_3 &lt;- target_date_1\n\n# Calculate daily new cases and deaths.\nco_centroid_covid_data &lt;- co_centroid_covid_data %&gt;% \n  group_by(fips) %&gt;% \n  arrange(fips, date) %&gt;% \n  mutate(\n    new_cases = cases - lag(cases),\n    new_deaths = deaths - lag(deaths)) %&gt;% \n  ungroup()\n\n# Calculate the weighted mean center for cases\nco_wm_cases &lt;- co_centroid_covid_data %&gt;% \n  filter(date == target_date_3) %&gt;% \n  group_by(date) %&gt;% \n  summarize(\n    lat_cases = weighted.mean(LAT, new_cases, na.rm = TRUE),\n    lon_cases = weighted.mean(LON, new_cases, na.rm = TRUE),\n    total_cases = sum(new_cases, na.rm = TRUE),\n  )\n\n# Calculate the weighted mean center for deaths\nco_wm_deaths &lt;- co_centroid_covid_data %&gt;% \n  filter(date == target_date_3) %&gt;% \n  group_by(date) %&gt;% \n  summarize(\n    lat_deaths = weighted.mean(LAT, new_deaths, na.rm = TRUE),\n    lon_deaths = weighted.mean(LON, new_deaths, na.rm = TRUE),\n    total_deaths = sum(new_deaths, na.rm = TRUE)\n  )\n\n\n# Create CO weighted mean cases plot\ncases_plot &lt;- ggplot(co_wm_cases) +\n  borders(\"state\", fill = \"gray90\", colour = \"white\") +   # Background US State map\n  geom_point(data = co_wm_cases, aes(x = lon_cases, y = lat_cases, size = total_cases), color = \"red\", alpha = 0.25) +    # Epicenters\n  scale_color_viridis_d() + #   Discrete color scale for time\n  theme_minimal() +\n  theme(legend.position = \"top\") +\n  labs(size = \"Cases\", x = \"\", y = \"\", title = \"Colorado COVID-19 Cases (Weighted Mean Center)\")\n\n# Create CO weighted mean deaths plot\ndeaths_plot &lt;- ggplot(co_wm_deaths) +\n  borders(\"state\", fill = \"gray90\", colour = \"white\") +   # Background US State map\n  geom_point(data = co_wm_deaths, aes(x = lon_deaths, y = lat_deaths, size = total_deaths), color = \"navy\", alpha = 0.25) +    # Epicenters\n  scale_color_viridis_d() + #   Discrete color scale for time\n  theme_minimal() +\n  theme(legend.position = \"top\") +\n  labs(size = \"Deaths\", x = \"\", y = \"\", title = \"Colorado COVID-19 Deaths (Weighted Mean Center)\")\n\n# Plot using patchwork to combine both cases and deaths visualizations\ncases_plot + deaths_plot + # Combine both plots\n  plot_annotation() # Fix horizontal squishing"
  },
  {
    "objectID": "labs/lab_06.html",
    "href": "labs/lab_06.html",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'rsample' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\nlibrary(ggplot2)\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.4.3\n\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3\n\nlibrary(tidymodels)\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(purrr)\nlibrary(kernlab)\n\n\nAttaching package: 'kernlab'\n\nThe following object is masked from 'package:scales':\n\n    alpha\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\nlibrary(rsample)\n\n# Download data\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\n# Download metadata and documentation\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf', mode = \"wb\")\n\nWarning in\ndownload.file(\"https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf\",\n: URL https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf:\ncannot open destfile 'data/camels_attributes_v2.0.pdf', reason 'No such file or\ndirectory'\n\n\nWarning in\ndownload.file(\"https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf\",\n: download had nonzero exit status\n\n# Get data specific text files\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Construct URLs and file names for the data\nremote_files &lt;- glue('{root}/camels_{types}.txt')\nlocal_files &lt;- glue('../data/lab_data/camels_{types}.txt')\n\n# Download specific data\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "labs/lab_06.html#lab-activity",
    "href": "labs/lab_06.html#lab-activity",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Lab Activity:",
    "text": "Lab Activity:\n\n# Model Preparation\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Create a scatter plot of aridity vs rainfall with log axes\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Scale the legend to the log scale plot\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Building the Model\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "labs/lab_06.html#question-3-deliverable-adjusted-wf-set",
    "href": "labs/lab_06.html#question-3-deliverable-adjusted-wf-set",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Question 3 Deliverable: Adjusted WF set",
    "text": "Question 3 Deliverable: Adjusted WF set\n\n# Data Validation\n# prep %&gt;% bake %&gt;% predict\ntest_data &lt;- bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n# Model Evaluation\n  #Statistical\n  metrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n  # Visual\n  ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n# Alternative Method: Workflow\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Adding other models to the workflow\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train)\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n# Adding xgboost\nxg_model &lt;- boost_tree() %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"regression\") \n\nxg_wf &lt;- workflow() %&gt;% \n  add_recipe(rec) %&gt;%       # Adding recipe\n  add_model(xg_model) %&gt;%   # Adding model\n  fit(data = camels_train)  # Fitting model\n\nxg_data &lt;- augment(xg_wf, new_data = camels_test)\ndim(xg_data)\n\n[1] 135  60\n\n# Adding an nnet\nnn_model &lt;- bag_mlp() %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\") \n\nnn_wf &lt;- workflow() %&gt;% \n  add_recipe(rec) %&gt;%       # Adding recipe\n  add_model(nn_model) %&gt;%   # Adding model\n  fit(data = camels_train)  # Fitting model\n\nnn_data &lt;- augment(nn_wf, new_data = camels_test)\ndim(xg_data)\n\n[1] 135  60\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.588\n2 rsq     standard       0.740\n3 mae     standard       0.365\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n# Workflowset approach\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, xg_model, nn_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.552  0.0292    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.782  0.0270    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.564  0.0255    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.770  0.0259    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, nn_model, xg_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nThe bagged MLP, neural network model appears to be the best with a mean r-sq of ~0.78 and a rmse of 0.57. The rsme is not the lowest but the r-sq is better than the rand forest at ~0.77. If rsme was a priority I would go with the rf model."
  },
  {
    "objectID": "labs/lab_06.html#question-4-deliverable",
    "href": "labs/lab_06.html#question-4-deliverable",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Question 4 Deliverable",
    "text": "Question 4 Deliverable\n\n# Set seed\nset.seed(6515)\ncamels &lt;- camels %&gt;% \n  mutate(logQmean = log(q_mean))   # Add logQmean column to df\n\n# Data Splitting\n## Generate split (75/25)\ncamels_split &lt;- initial_split(camels, prop = 0.75)\n  ## Extract training and testing sets\n  camels_tr &lt;- training(camels_split)\n  camels_te  &lt;- testing(camels_split)\n  ## 10-fold CV dataset\n  camels_10cv &lt;- vfold_cv(camels_tr, v = 10)\n  \n# Recipe Vars Review\n  # Check for skewing\ncamels_tr %&gt;%\n  select(pet_mean, p_mean, runoff_ratio, baseflow_index, aridity, slope_mean, area_geospa_fabric) %&gt;%\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(~ variable, scales = \"free\") +\n  theme_minimal()\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n# Recipie\nalt_rec &lt;- recipe(logQmean ~ pet_mean + p_mean + aridity + runoff_ratio + baseflow_index + slope_mean + area_geospa_fabric, data = camels_tr) %&gt;% \n  step_YeoJohnson(all_predictors()) %&gt;% \n  step_interact(terms = ~ pet_mean:p_mean + aridity:runoff_ratio + area_geospa_fabric:slope_mean) %&gt;% \n  step_corr(all_predictors(), threshold = 0.9) %&gt;%   # Remove highly correlated predictors to avoid multicollinearity.\n  step_normalize(all_predictors()) %&gt;% \n  step_naomit(all_predictors(), all_outcomes())\n  \n# Define and Train Models\n  ## Define rf model\n  rf_alt_model &lt;- rand_forest() %&gt;% \n    set_engine(\"ranger\") %&gt;% \n    set_mode(\"regression\")\n  \n  rf_alt_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(alt_rec) %&gt;%\n    # Add the model\n    add_model(rf_alt_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n   \n  rf_predictions &lt;- augment(rf_alt_wf, new_data = camels_te) \n\n  ## Define xg model\n  xg_alt_model &lt;- boost_tree() %&gt;% \n    set_engine(\"xgboost\") %&gt;% \n    set_mode(\"regression\")\n  \n  xg_alt_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(alt_rec) %&gt;%\n    # Add the model\n    add_model(xg_alt_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n  \n  xg_predictions &lt;- augment(xg_alt_wf, new_data = camels_te)\n  \n  ## Define nueral net model\n  nn_alt_model &lt;- bag_mlp() %&gt;% \n    set_engine(\"nnet\") %&gt;% \n    set_mode(\"regression\")\n  \n  nn_alt_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(alt_rec) %&gt;%\n    # Add the model\n    add_model(nn_alt_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n  \n  nn_predictions &lt;- augment(nn_alt_wf, new_data = camels_te)\n  \n  ## Define linear reg model\n  lm_alt_model &lt;- linear_reg() %&gt;% \n    set_engine(\"lm\") %&gt;% \n    set_mode(\"regression\")\n  \n  lm_alt_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(alt_rec) %&gt;%\n    # Add the model\n    add_model(lm_alt_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)\n  \n  lm_predictions &lt;- augment(lm_alt_wf, new_data = camels_te) \n  \n  ## Define SVM-nonlinear model\n  svm_alt_model &lt;- svm_rbf() %&gt;% \n    set_engine(\"kernlab\") %&gt;% \n    set_mode(\"regression\")\n\n  svm_alt_wf &lt;- workflow() %&gt;%\n    # Add the recipe\n    add_recipe(alt_rec) %&gt;%\n    # Add the model\n    add_model(svm_alt_model) %&gt;%\n    # Fit the model\n    fit(data = camels_tr)  \n  \n  svm_predictions &lt;- augment(svm_alt_wf, new_data = camels_te)\n  \n # Implement workflowset analysis\n  \n  alt_wf_set &lt;- workflow_set(preproc = list(rec),\n                          models = list(rf = rf_alt_model, \n                                        xg = xg_alt_model, \n                                        nn = nn_alt_model, \n                                        lm = lm_alt_model, \n                                        svm = svm_alt_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_10cv) \n  \nautoplot(alt_wf_set)\n\n\n\n\n\n\n\nrank_results(alt_wf_set, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 10 × 9\n   wflow_id   .config       .metric  mean std_err     n preprocessor model  rank\n   &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n 1 recipe_nn  Preprocessor… rmse    0.565  0.0347    10 recipe       bag_…     1\n 2 recipe_nn  Preprocessor… rsq     0.771  0.0149    10 recipe       bag_…     1\n 3 recipe_lm  Preprocessor… rmse    0.571  0.0426    10 recipe       line…     2\n 4 recipe_lm  Preprocessor… rsq     0.765  0.0222    10 recipe       line…     2\n 5 recipe_svm Preprocessor… rmse    0.561  0.0414    10 recipe       svm_…     3\n 6 recipe_svm Preprocessor… rsq     0.760  0.0204    10 recipe       svm_…     3\n 7 recipe_rf  Preprocessor… rmse    0.576  0.0397    10 recipe       rand…     4\n 8 recipe_rf  Preprocessor… rsq     0.754  0.0188    10 recipe       rand…     4\n 9 recipe_xg  Preprocessor… rmse    0.610  0.0371    10 recipe       boos…     5\n10 recipe_xg  Preprocessor… rsq     0.724  0.0161    10 recipe       boos…     5\n\n# Moving forward with the NN Model\n\n  ## Extract the model coefficients\n  nn_coeff &lt;- coef(nn_alt_model)  \n  nn_coeff\n\nNULL\n\n  ## Use the data to make predictions\n  metrics(nn_predictions, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      0.0615\n2 rsq     standard      0.998 \n3 mae     standard      0.0111\n\n  ggplot(nn_predictions, aes(x = logQmean, y = .pred)) +\n    geom_point(aes(color = .pred), size = 2) +\n    scale_color_gradient(low = \"tan\", high = \"royalblue\") +\n    labs(title = \"Observed vs Predicted Values with the NN Model\",\n         x = \"Observed Log Mean Flow\",\n         y = \"Predicted Log Mean Flow\",\n         color = \"Aridity\") +\n    geom_abline(linetype = 2) +\n    theme_linedraw()\n\n\n\n\n\n\n\n\nQ4b: I chose a complex formula to attempt to compute multiple elements of the watershed. Temperature (pet), precipitation (p) and aridity are all related as is runoff, slope and catchment area (area_geo…). Finally I included baseflow index because it seems like predicting flow will be challenging without first understanding what water is already there from groundwater sources, other inputs, etc.\nQ4c: I used selected the above models in an attempt to find non-linear representations for the data. Given the complexity of the formula I made for my recipe I anticipated the data fitting linear models poorly.\nQ4e: For the recipe I created, the bag_mlp neural network model performed the best. It had the highest mean r-squared (~0.771) and the second lowest root mean standard error (rmse) (~0.565). The SVM model had a marginally lower rmse (~0.561) but also had a lower mean r-squared (~0.0760). If I really wanted to I could tune the SVM and NN models to optimize them and then compare them again.\nQ4f: I am very happy with the results, outside of a few outliers, my models seems to have improve on the performance of the recipe and models from part 3."
  },
  {
    "objectID": "labs/lab_10.html",
    "href": "labs/lab_10.html",
    "title": "Lab 10: Distances and Projections",
    "section": "",
    "text": "# Install Reference Data\nremotes::install_github(\"ropensci/USAboundaries\")\n\nUsing GitHub PAT from the git credential store.\n\n\nSkipping install of 'USAboundaries' from a github remote, the SHA1 (0f56f492) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nremotes::install_github(\"ropensci/USAboundariesData\")\n\nUsing GitHub PAT from the git credential store.\n\n\nSkipping install of 'USAboundariesData' from a github remote, the SHA1 (064cdbcb) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nremotes::install_github(\"mikejohnson51/AOI\")\n\nUsing GitHub PAT from the git credential store.\n\n\nSkipping install of 'AOI' from a github remote, the SHA1 (f821d499) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nremotes::install_github(\"ropenscilabs/rnaturalearthdata\")\n\nUsing GitHub PAT from the git credential store.\n\n\nSkipping install of 'rnaturalearthdata' from a github remote, the SHA1 (ff4d891f) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\n\n# Load libraries\n# spatial data science\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.4.3\n\n\nLinking to GEOS 3.13.0, GDAL 3.10.1, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(units)\n\nWarning: package 'units' was built under R version 4.4.3\n\n\nudunits database from C:/Users/Zacha/AppData/Local/R/win-library/4.4/units/share/udunits/udunits2.xml\n\n# Data\nlibrary(AOI)\n\n# Visualization\nlibrary(gghighlight)\n\nWarning: package 'gghighlight' was built under R version 4.4.3\n\nlibrary(ggrepel)\n\nWarning: package 'ggrepel' was built under R version 4.4.3\n\nlibrary(knitr)\n\nWarning: package 'knitr' was built under R version 4.4.3\n\n\n\n# 1.1: Define a projection\neqdc &lt;- '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\neqcd &lt;- st_crs(5070) # sects"
  },
  {
    "objectID": "daily_exercises/exercise_05.html",
    "href": "daily_exercises/exercise_05.html",
    "title": "Daily Exercise 05",
    "section": "",
    "text": "# Attach the `palmerspenguins` package\n  library(palmerpenguins)\n\n# 1. Examine at the dataset using the ?Help page\n  #?penguins\n\n# 2. what is the class of the penguins dataset?\n  class(penguins)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# 3. what is the structure of the penguins dataset?\n  str(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n# 4. what are the dimensions of the penguins dataset?\n  #Rows\n  dim(penguins)[1]\n\n[1] 344\n\n  #Columns\n  dim(penguins)[2]\n\n[1] 8\n\n# 5. what are the column names of the penguins dataset?\n  colnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n# 6. what type of data is `flipper_length_mm` and `Island`?\n  #flipper_length_mm: \n  class(penguins$flipper_length_mm)\n\n[1] \"integer\"\n\n  #Island: \n  class(penguins$island)\n\n[1] \"factor\"\n\n# 7. what is the mean flipper length of the penguins?\n  mean(penguins$flipper_length_mm, na.rm = TRUE)\n\n[1] 200.9152\n\n# 8. what is the standard deviation of flipper length in the penguins?\n  sd (penguins$flipper_length_mm, na.rm = TRUE)\n\n[1] 14.06171\n\n# 9. what is the median body mass of the penguins?\n  median(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 4050\n\n# 10. what is the Island of the 100th penguin?\n  penguins$island[100]\n\n[1] Dream\nLevels: Biscoe Dream Torgersen"
  }
]